

æœ‰äº†GPT ä¹‹å, æƒ³ç³»ç»Ÿå­¦ä¹ ä¹Ÿè®¸åªéœ€è¦ä¸€ä¸ªç›®å½•,  ä¸æ–­é—®GPT ç»†èŠ‚å°±è¡Œ. 



## huggingface NLP course

### ç¬¬äºŒç« 

2-2 

https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt

padæ˜¯å› ä¸ºä¸åŒçš„batch ä¸ä¸€æ ·é•¿. attention maskä¹Ÿä¼šè®¾ä¸º0 è¡¨ç¤ºè¿™äº›æ˜¯padding çš„.

è¾“å‡ºçš„logitsä¸æ˜¯æ¦‚ç‡,  softmax ä¹‹åå˜æˆæ¦‚ç‡ 

head æ˜¯ä»€ä¹ˆ?   The model heads take the high-dimensional vector of hidden states as input and project them onto logits.  è¾“å‡ºdimæ˜¯vocab size å°±æ˜¯å‡ ä¸ªlinear.  å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ¶æ„æ‰§è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œä½†æ¯ä¸ªä»»åŠ¡éƒ½å°†å…·æœ‰ä¸åŒçš„ head ä¸ä¹‹å…³è”ã€‚  Transformer æ¨¡å‹çš„è¾“å‡ºç›´æ¥å‘é€åˆ°æ¨¡å‹å¤´è¿›è¡Œå¤„ç†.  çœ‹å›¾  https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg  

æ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹éƒ½è¾“å‡º logitsï¼Œå› ä¸ºç”¨äºè®­ç»ƒçš„æŸå¤±å‡½æ•°é€šå¸¸ä¼šå°†æœ€åä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ SoftMaxï¼‰ä¸å®é™…çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰èåˆ

quiz: 

- lossæ˜¯æ€ä¹ˆç®—çš„?

- logitsæ˜¯ä»€ä¹ˆ?


è®­ç»ƒè‡ªå›å½’æ¨¡å‹è¿˜æ˜¯åº”è¯¥ç”¨forwardè€Œä¸æ˜¯generate.

2-3 models

config loadçš„æ—¶å€™å¯ä»¥ç›´æ¥æ”¹config. 

2-4 tokenizers 

todo : ç”¨åˆ°çš„æ—¶å€™ä»”ç»†çœ‹çœ‹

2-5 Handling multiple sequences

paddingå¿…é¡»å’Œ attention maskç»„åˆä½¿ç”¨ä¸ç„¶å°±å‡ºé”™. 

2-6tokenizers åº“

éå¸¸å¼ºå¤§, 

```
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")# å¯ä»¥è¿”å›torch, TensorFlowå’Œnumpyæ•°ç»„. 

model_inputs = tokenizer(sequence) åˆ†è¯å™¨åœ¨å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šå•è¯ [CLS]ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šå•è¯ [SEP]ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ç”¨è¿™äº›è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå› æ­¤ä¸ºäº†è·å¾—ç›¸åŒçš„æ¨ç†ç»“æœéœ€è¦æ·»åŠ . 
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence) # å’Œç›´æ¥è°ƒç”¨ä¸ä¸€æ ·!æ²¡æœ‰CLSäº†. 
ids = tokenizer.convert_tokens_to_ids(tokens) #å°±å¯ä»¥çœ‹æ•°å­—äº†.
print(tokenizer.decode(model_inputs["input_ids"])) #å°±å¯ä»¥çœ‹textäº†. ç”¨çš„æ˜¯åŒä¸€ä¸ªtokenizer


ç‰¹æ®Štoken:
llama2ç­‰æ¨¡å‹çš„ä¸­<s>æ˜¯ BOS (beginning of a sentence) token
[CLS] ä»£è¡¨åˆ†ç±»ã€‚ä¹‹æ‰€ä»¥åœ¨å¼€å¤´æ·»åŠ ï¼Œæ˜¯å› ä¸ºè¿™é‡Œçš„è®­ç»ƒä»»åŠ¡æ˜¯å¥å­åˆ†ç±»ã€‚ç”±äºä»–ä»¬éœ€è¦å¯ä»¥è¡¨ç¤ºæ•´ä¸ªå¥å­å«ä¹‰çš„è¾“å…¥ï¼Œå› æ­¤ä»–ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ ‡ç­¾ã€‚
0 <unk>,  1 <s>, 2 </s>, 
æœ‰çš„ä¼šæ’å…¥ im_start 
tokenizer.special_tokens_map çœ‹
```

3-2 

```
raw_datasets = load_dataset("glue", "mrpc") 
# todo , glueæ˜¯å•¥æ„æ€? æ²¡æ‰¾åˆ°è¿™ä¸ªåº“

raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0] #çœ‹é•¿å•¥æ ·

raw_train_dataset.features  # è¿™å°†å‘Šè¯‰æˆ‘ä»¬æ¯åˆ—çš„ç±»å‹ï¼š
label çš„ç±»å‹ä¸º ClassLabelï¼Œæ•´æ•°åˆ°æ ‡ç­¾ name çš„æ˜ å°„å­˜å‚¨åœ¨ names æ–‡ä»¶å¤¹ä¸­

tokenizerè¿˜å¯ä»¥ç”¨é€—å·ä¼ å…¥ä¸¤ä¸ª, è¿”å›token type idsæ¥è¡¨æ˜æ˜¯ç¬¬ä¸€ä¸ªè¿˜æ˜¯ç¬¬äºŒä¸ª.(è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨é€‰æ‹©å…¶ä»–æ£€æŸ¥ç‚¹ï¼Œåˆ™æ ‡è®°åŒ–è¾“å…¥ä¸­ä¸ä¸€å®šåŒ…å«token_type_idsï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨ DistilBERT æ¨¡å‹ï¼Œåˆ™ä¸ä¼šè¿”å›å®ƒä»¬ï¼‰ã€‚åªæœ‰å½“æ¨¡å‹çŸ¥é“å¦‚ä½•å¤„ç†å®ƒä»¬æ—¶ï¼Œæ‰ä¼šè¿”å›å®ƒä»¬ï¼Œå› ä¸ºå®ƒåœ¨é¢„è®­ç»ƒæœŸé—´å·²ç»çœ‹åˆ°äº†å®ƒä»¬ã€‚) è¿˜å¯ä»¥ä¼ å…¥å¤šä¸ªåºåˆ—.


ä¸ºäº†å°†æ•°æ®ä¿å­˜ä¸ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Dataset.mapï¼ˆï¼‰ æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦å®Œæˆæ›´å¤šçš„é¢„å¤„ç†è€Œä¸ä»…ä»…æ˜¯ tokenizationï¼Œè¿™ä¹Ÿä¸ºæˆ‘ä»¬æä¾›äº†ä¸€äº›é¢å¤–çš„çµæ´»æ€§ã€‚

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
è¿™å°†å…è®¸æˆ‘ä»¬åœ¨è°ƒç”¨ mapï¼ˆï¼‰ æ—¶ä½¿ç”¨é€‰é¡¹ batched=Trueï¼Œè¿™å°†å¤§å¤§åŠ å¿«åˆ†è¯é€Ÿåº¦ã€‚åˆ†è¯å™¨ç”± Tokenizers åº“ä¸­ç”¨ Rust ç¼–å†™çš„ğŸ¤—åˆ†è¯å™¨æä¾›æ”¯æŒã€‚è¿™ä¸ªåˆ†è¯å™¨å¯ä»¥éå¸¸å¿«ï¼Œä½†å‰ææ˜¯æˆ‘ä»¬ä¸€æ¬¡ç»™å®ƒå¾ˆå¤šè¾“å…¥ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨åœ¨ tokenization å‡½æ•°ä¸­çœç•¥äº† padding å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºå°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦æ•ˆç‡ä¸é«˜ï¼šæœ€å¥½åœ¨æ„å»ºæ‰¹æ¬¡æ—¶å¡«å……æ ·æœ¬ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……è¯¥æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥çš„é•¿åº¦éå¸¸å¯å˜æ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›ï¼

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)


tok("dog walked", add_special_tokens=True)) çš„è®²è§£  https://github.com/huggingface/transformers/issues/22935
å½“è®¾ç½®ä¸º True æ—¶ï¼Œadd_special_tokensç”¨äºåœ¨è¾“å…¥åºåˆ—çš„å¼€å¤´å’Œç»“å°¾æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚åœ¨æ‚¨çš„æƒ…å†µä¸‹ï¼Œç”±äºæ‚¨ä½¿ç”¨çš„æ˜¯å•ä¸ªè¾“å…¥åºåˆ—ï¼Œå› æ­¤åˆ†è¯å™¨å°†åˆ†åˆ«åœ¨å¥å­çš„å¼€å¤´å’Œç»“å°¾æ·»åŠ ç‰¹æ®Šæ ‡è®° [CLS] å’Œ [SEP]ã€‚
è¯·æ³¨æ„ï¼Œå¹¶éæ‰€æœ‰åˆ†è¯å™¨éƒ½æ”¯æŒæ·»åŠ ç‰¹æ®Šåˆ†è¯ã€‚å¦‚æœåˆ†è¯å™¨ä¸æ”¯æŒæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œåˆ™å°† add_special_tokens è®¾ç½®ä¸º True å°†ä¸èµ·ä½œç”¨ã€‚


# todo
ç»§ç»­é˜…è¯»åé¢å†…å®¹, Dynamic padding  
```

3-3 trainer

todo

```
Trainer é‡Œé¢æœ‰ä¸€ä¸ªæ–¹æ³•å« compute_lossï¼š

é»˜è®¤å®ç°æ˜¯è°ƒç”¨ model(**inputs) å¾—åˆ° outputsï¼Œç„¶åä» outputs é‡Œå– lossã€‚
ä½ å¯ä»¥è‡ªå®šä¹‰ Trainerï¼Œé‡å†™ compute_lossï¼Œæ¯”å¦‚ç›´æ¥è°ƒç”¨è‡ªå®šä¹‰çš„ loss è®¡ç®—å‡½æ•°

def compute_loss(self, model, inputs, return_outputs=False):
    outputs = model(**inputs)  # è¿™é‡Œå…¶å®å°±æ˜¯ forward
    loss = outputs["loss"]     # æˆ–è€…ä½ å¯ä»¥è‡ªå®šä¹‰æ€ä¹ˆå– loss
    return (loss, outputs) if return_outputs else loss
```

è¾“å‡ºå¤ªå¤šäº†, ä½†æ˜¯ `disable_tqdm=True` ä¼šå¯¼è‡´ `Trainer` ä¸è§¦å‘ `on_log`ï¼Œä»è€Œ wandb ä¸è®°å½•ä»»ä½•æ—¥å¿—ã€‚  æ‰€ä»¥è¿˜éœ€è¦æ‰‹åŠ¨log

æœ‰trainer, wandb åœ¨shellé‡Œé¢åˆå§‹åŒ–æœ€æ–¹ä¾¿, ä¸ç”¨ä¿®æ”¹ä»»ä½•pythonä»£ç  .æ²¡æœ‰trainerå°±è¦åŠ å¾ˆå¤šwandb çš„ä»£ç .

```
export WANDB_PROJECT=
export WANDB_ENTITY=
 trainer ä¼šè‡ªåŠ¨è¯»å–è¿™ä¸¤ä¸ª. 
```







 

3-4 A full training 

todo

```

trainer.save_stateï¼ˆï¼‰ æ˜¯ä¸€ç§è´Ÿè´£ä¿å­˜è®­ç»ƒå¾ªç¯æœ¬èº«çš„å½“å‰çŠ¶æ€çš„æ–¹æ³•ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¨¡å‹æƒé‡ã€‚è¿™å¯¹äºèƒ½å¤Ÿä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹æ¢å¤è®­ç»ƒè‡³å…³é‡è¦ã€‚


ä¼˜åŒ–å™¨çŠ¶æ€ï¼š ä¿å­˜ä¼˜åŒ–å™¨çš„å†…éƒ¨çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼ŒAdam/AdamW çš„åŠ¨é‡ç¼“å†²åŒºã€è¿è¡Œå¹³å‡å€¼ç­‰ï¼‰ã€‚å¦‚æœæ‚¨åœ¨æ²¡æœ‰æ­¤åŠŸèƒ½çš„æƒ…å†µä¸‹é‡æ–°å¼€å§‹è®­ç»ƒï¼Œä¼˜åŒ–å™¨å°†ä»å¤´å¼€å§‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„æ”¶æ•›è¡Œä¸ºã€‚é€šå¸¸ä¿å­˜ä¸º optimizer.ptã€‚
```

### ç¬¬ä¸ƒ

7-3

æ‚¨è¿˜å¯ä»¥æ£€æŸ¥ `train` å’Œ `test` splits ä¸­çš„æ ‡ç­¾æ˜¯å¦ç¡®å®æ˜¯ `0` æˆ– `1` â€” è¿™æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å¥å…¨æ€§æ£€æŸ¥ï¼Œæ¯ä¸ª NLP ä»ä¸šè€…éƒ½åº”è¯¥åœ¨æ–°é¡¹ç›®å¼€å§‹æ—¶æ‰§è¡Œ

å¯¹äºè‡ªå›å½’å’Œæ©ç è¯­è¨€å»ºæ¨¡ï¼Œä¸€ä¸ªå¸¸è§çš„é¢„å¤„ç†æ­¥éª¤æ˜¯è¿æ¥æ‰€æœ‰ç¤ºä¾‹ï¼Œç„¶åå°†æ•´ä¸ªè¯­æ–™åº“æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰çš„å—ã€‚è¿™ä¸æˆ‘ä»¬é€šå¸¸çš„æ–¹æ³•å®Œå…¨ä¸åŒï¼Œåœ¨é€šå¸¸çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯ç®€å•åœ°å¯¹å•ä¸ªç¤ºä¾‹è¿›è¡Œæ ‡è®°ã€‚ä¸ºä»€ä¹ˆè¦æŠŠæ‰€æœ‰ä¸œè¥¿éƒ½è¿æ¥åœ¨ä¸€èµ·å‘¢ï¼ŸåŸå› æ˜¯å¦‚æœå•ä¸ªç¤ºä¾‹å¤ªé•¿ï¼Œå®ƒä»¬å¯èƒ½ä¼šè¢«æˆªæ–­ï¼Œè¿™ä¼šå¯¼è‡´ä¸¢å¤±å¯èƒ½å¯¹è¯­è¨€å»ºæ¨¡ä»»åŠ¡æœ‰ç”¨çš„ä¿¡æ¯.  æ‰€ä»¥ä¸åœ¨åˆ†è¯å™¨ä¸­è®¾ç½® `truncation=True`

Weâ€™ll also grab the word IDs if they are available ((which they will be if weâ€™re using a fast tokenizer, as described in [Chapter 6](https://huggingface.co/course/chapter6/3)), as we will need them later on to do whole word masking.   å•¥æ„æ€? 

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))] # word idå°±æ˜¯é™¤å»ç‰¹æ®Šå­—ç¬¦çš„æ¯ä¸ªtokençš„é¡ºåº. 
    return result

# Use batched=True to activate fast multithreading!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)

ä¸ºä»€ä¹ˆè¦åœ¨æ¯å¥è¯åŠ ä¸Š[SEP] å’Œ[CLS]?  å› ä¸ºæ˜¯å¤šä¸ªexamplesæ”¾åœ¨ä¸€ä¸ªchunké‡Œäº†
return_overflowing_tokensæ˜¯æ€ä¹ˆç”¨çš„? 
```

ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹ç”µå½±è¯„è®ºè¿›è¡Œäº†æ ‡è®°åŒ–ï¼Œä¸‹ä¸€æ­¥æ˜¯å°†å®ƒä»¬å…¨éƒ¨åˆ†ç»„åœ¨ä¸€èµ·å¹¶å°†ç»“æœæ‹†åˆ†ä¸ºå—ã€‚ä½†æ˜¯è¿™äº›å—åº”è¯¥æœ‰å¤šå¤§å‘¢ï¼Ÿè¿™æœ€ç»ˆå°†ç”±æ‚¨å¯ç”¨çš„ GPU å†…å­˜é‡å†³å®šï¼Œä½†ä¸€ä¸ªå¥½çš„èµ·ç‚¹æ˜¯æŸ¥çœ‹æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°æ˜¯å¤šå°‘ã€‚`tokenizer.model_max_length`  

åœ¨å®é™…åœºæ™¯ä¸­ä½¿ç”¨è¾ƒå°çš„æ•°æ®å—å¤§å°å¯èƒ½æ˜¯æœ‰å®³çš„ï¼Œå› æ­¤æ‚¨åº”è¯¥ä½¿ç”¨ä¸æ‚¨å°†åº”ç”¨æ¨¡å‹çš„ç”¨ä¾‹ç›¸å¯¹åº”çš„å¤§å°ã€‚

æœ‰çš„ä¹Ÿæœ‰ç‚¹è€äº†, éƒ½è¿˜åœ¨ç”¨p100 å’Œbert.



7-6 Training a causal language model from scratch

mask æ˜¯ä»€ä¹ˆ? è¿™é‡Œæ²¡æœ‰è®²è¿™äº›ç»†èŠ‚.  éƒ½ç”¨ DataCollatorForLanguageModeling åŒ…è£…äº†, `data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)` å°±æ˜¯å› æœå»ºæ¨¡

æ¸…æ´—æ•°æ®: 
å¯¹æ›´å¤šä½¿ç”¨è¿™äº›åº“çš„è®­ç»ƒæ ·æœ¬ç»™äºˆæ›´å¤šæƒé‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ pltã€pdã€skã€fit å’Œ predict ç­‰å…³é”®å­—è½»æ¾è¯†åˆ«è¿™äº›ç¤ºä¾‹ï¼Œè¿™äº›å…³é”®å­—æ˜¯ matplotlib.pyplotã€pandas å’Œ sklearn æœ€å¸¸è§çš„å¯¼å…¥åç§°ï¼Œä»¥åŠåè€…çš„ fit/predict æ¨¡å¼ã€‚å¦‚æœå®ƒä»¬éƒ½è¡¨ç¤ºä¸ºå•ä¸ª tokenï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾æ£€æŸ¥å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨ input åºåˆ—ä¸­ã€‚æ ‡è®°å¯ä»¥å…·æœ‰ç©ºæ ¼å‰ç¼€ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å°†åœ¨ tokenizer è¯æ±‡è¡¨ä¸­æ£€æŸ¥è¿™äº›ç‰ˆæœ¬ã€‚

casual llm  forwardæ€ä¹ˆå†™?

åº•å±‚åŸç†

```
    shift_labels = inputs[..., 1:].contiguous() 
æˆ‘ä»¬éœ€è¦å¯¹é½ logits å’Œ inputsï¼šå‘å³ç§»åŠ¨ 1 çš„ input åºåˆ—å½¢æˆæ ‡ç­¾ï¼Œå› ä¸ºä¸‹ä¸€ä¸ª token æ˜¯å½“å‰ token çš„æ ‡ç­¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»è¾“å…¥åºåˆ—çš„ç¬¬äºŒä¸ªæ ‡è®°å¼€å§‹æ ‡è®°æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæ¨¡å‹æ— è®ºå¦‚ä½•éƒ½ä¸ä¼šå¯¹ç¬¬ä¸€ä¸ªæ ‡è®°è¿›è¡Œé¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬åˆ‡æ–­æœ€åä¸€ä¸ª logitï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰éµå¾ªå®Œæ•´ input åºåˆ—çš„ token çš„æ ‡ç­¾ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±.
```

input idå’Œlabelsæ˜¯å¯¹é½çš„, åœ¨causal LMä¸­ä¼šè‡ªåŠ¨åç§»ç®—loss.  æ‰€ä»¥æˆ‘ä»¬ä¸ç”¨å†™ä¸Šé¢çš„æ“ä½œ. 



#### generate

å¤§éƒ¨åˆ†æ¨¡å‹ ç»§æ‰¿äº†PreTrainedModel , ç»§æ‰¿äº†GenerationMixin, è°ƒç”¨generate, å°±å¯ä»¥æŠŠself è‡ªå·±ä¼ è¿›å», è¿™é‡Œæ˜¯è°ƒç”¨äº† WhisperForConditionalGeneration ä¸­çš„forwardå‡½æ•°ã€‚è¿™æ˜¯å› ä¸º PyTorch çš„ nn.Module åŸºç±»å®šä¹‰äº†ä¸€ä¸ª __call__ æ–¹æ³•ï¼Œå½“ä½ è°ƒç”¨æ¨¡å‹å®ä¾‹ï¼ˆå³ selfï¼‰æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è°ƒç”¨è¿™ä¸ª __call__ æ–¹æ³•ï¼Œè€Œè¿™ä¸ª __call__ æ–¹æ³•åˆä¼šè°ƒç”¨ forward æ–¹æ³•ã€‚

https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0005

#### ç”Ÿæˆçš„è¾“å‡º

https://huggingface.co/docs/transformers/en/internal/generation_utils

å¦‚æœæ¨¡å‹å°šæœªè¿”å›è¯¥å±æ€§ï¼Œæ‚¨å°†å¾—åˆ° `None`, ä¸ä¼šæŠ¥é”™. 

`logits` æ˜¯ **æ¨¡å‹çš„åŸå§‹è¾“å‡º**ï¼Œæœªç»ä¿®æ”¹ã€‚

`scores` æ˜¯ **ç”¨äºç”Ÿæˆ token çš„å¤„ç†åçš„åˆ†æ•°**ï¼Œå¯èƒ½å·²ç»è¿‡æ¸©åº¦ç¼©æ”¾æˆ–å…¶ä»–å¤„ç†ã€‚

åœ¨æ ‡å‡† `greedy` æˆ– `beam search` ç”Ÿæˆæ—¶ï¼Œ`scores` å’Œ `logits` å¯èƒ½ç›¸åŒï¼›ä½†åœ¨ **Top-k / Top-p / æ¸©åº¦é‡‡æ ·** ç­‰æƒ…å†µä¸‹ï¼Œ`scores` å¯èƒ½ä¸ `logits` æœ‰æ˜¾è‘—åŒºåˆ«.

 When you set `output_hidden_states=True` and `return_dict_in_generate=True`, the `language_model_output.hidden_states` will be a tuple of tuples containing the hidden states for each generation step.  å°±æ˜¯è¾“å‡º8ä¸ªtoken, ä¼šè¾“å‡ºä¸€ä¸ª ( 8 è¾“å‡ºtokenæ•°,33 å±‚æ•°, hidden size) è¿™æ ·ä¸€ä¸ª Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.  

æ˜¯ä¸æ˜¯å°±ç®—shapeä¸å¯¹, ä½†æ˜¯

Hidden statesæ€ä¹ˆappendçš„ï¼Ÿ

è¦çœ‹çœ‹åŸå§‹ä»£ç , 

è¾“å…¥ inputs_embeds è€Œä¸æ˜¯ input_ids,    outputs.sequences ä¸åŒ…å«è¾“å…¥çš„ input_ids.

`tokenizer.add_special_tokens(...)` ä¼šæŠŠ `<ACT>` åŠ å…¥ vocab

**ä½† model çš„ embedding å±‚å¤§å°ä¸ä¼šè‡ªåŠ¨æ›´æ–°**

HuggingFace çš„ `resize_token_embeddings()` é»˜è®¤è¡Œä¸ºåªå¤„ç† **input embedding**ã€‚
 è€Œ **output embedding** å¯èƒ½æ˜¯ï¼š

1. è¢« `tie_weights()` å…±äº«ï¼Œæˆ–è€…
2. æ˜¯ä¸€ä¸ªå•ç‹¬çš„ `Linear` å±‚ æ‰€ä»¥éœ€è¦æ‰‹åŠ¨å¹³å‡

````
    model.resize_token_embeddings(len(tokenizer))
    output_embeddings = model.language_model.get_output_embeddings().weight.data
    output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)
    output_embeddings[-num_new_tokens:] = output_embeddings_avg
````



## attentionmask é—®é¢˜

å¯ä»¥çœ‹çœ‹   torchtuneçš„æ–‡æ¡£. 

https://pytorch.org/torchtune/stable/_modules/torchtune/generation/_generation.html#get_causal_mask_from_padding_mask

æ¨ç†çš„æ—¶å€™éœ€è¦ attention_mask å—? 

**å¦‚æœ inputs_embeds æ˜¯ padding åçš„å˜é•¿è¾“å…¥**

- LLaMA 2 é»˜è®¤ä¸å¤„ç† `padding`ï¼Œæ‰€ä»¥ **éœ€è¦æä¾› attention_maskï¼Œç¡®ä¿æ¨¡å‹å¿½ç•¥å¡«å……éƒ¨åˆ†**ã€‚

**å¤šæ¨¡æ€è¾“å…¥åœºæ™¯ï¼ˆå¦‚ CLIP + LLaMAï¼‰**

- ä½ å¯èƒ½éœ€è¦ `attention_mask`ï¼Œä»¥åŒºåˆ† **æ–‡æœ¬éƒ¨åˆ†** å’Œ **å¤šæ¨¡æ€éƒ¨åˆ†**ï¼ˆå¦‚å›¾åƒ embeddings

æ‰‹åŠ¨æ§åˆ¶æ¨¡å‹æ³¨æ„åŠ›èŒƒå›´,  å¦‚æœä½ æƒ³è®©æ¨¡å‹å…³æ³¨ **ç‰¹å®š token**ï¼ˆå¦‚åªå¯¹æ–‡æœ¬éƒ¨åˆ†åšæ³¨æ„åŠ›è®¡ç®—ï¼‰ï¼Œå¯ä»¥æ‰‹åŠ¨æä¾› `attention_mask`ã€‚ **LLaMA 2 æ˜¯ decoder-only Transformer**ï¼Œé»˜è®¤ä½¿ç”¨ ï¼Œå³ï¼š å› æœï¼ˆcausalï¼‰masking , è‡ªåŠ¨å±è”½æœªæ¥ tokensï¼Œç¡®ä¿æ¨¡å‹åªèƒ½çœ‹åˆ°è¿‡å»çš„ä¿¡æ¯ã€‚ è¿™ç§ mask åœ¨ generate() è¿‡ç¨‹ä¸­æ˜¯ éšå¼æ·»åŠ çš„ï¼Œä¸éœ€è¦é¢å¤–ä¼  attention_maskã€‚ æºç ä¼šè‡ªåŠ¨å±è”½ pad id  å¦‚æœä½ ç›´æ¥è°ƒç”¨ `self.language_model()`ï¼ˆå³ä¸ä½¿ç”¨ `.generate()`ï¼Œè€Œæ˜¯ç›´æ¥å‰å‘ä¼ æ’­ `forward` æ–¹æ³•ï¼‰ï¼Œ**é€šå¸¸éœ€è¦æä¾› attention_mask**ï¼Œå…·ä½“æƒ…å†µå¦‚ä¸‹.

å¯¹äºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆmasked language modelï¼Œå¦‚BERTï¼‰ï¼Œself.language_modelç¡®å®å¯ä»¥ä¸€æ¬¡é¢„æµ‹æ‰€æœ‰è¢«æ©ç çš„tokenã€‚è¿™æ˜¯æ©ç è¯­è¨€æ¨¡å‹ä¸è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªé‡è¦åŒºåˆ«ã€‚

ç›´æ¥ä½¿ç”¨self.language_modelä¸èƒ½åŸç”Ÿåœ°ä¸€æ¬¡è¾“å‡º8ä¸ªtoken.   éœ€è¦ç¼–å†™é¢å¤–çš„å¾ªç¯å’Œæ§åˆ¶é€»è¾‘ã€‚ .  self.language_modelé€šå¸¸æ˜¯å‰å‘ä¼ æ’­çš„æ ¸å¿ƒå®ç°ï¼Œæ¯æ¬¡è°ƒç”¨åªä¼šæ ¹æ®è¾“å…¥ä¸Šä¸‹æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ.

è‡ªå›å½’å’Œæ©ç è¯­è¨€æ¨¡å‹åŒºåˆ«?

ä¸€æ¬¡é¢„æµ‹æ‰€æœ‰è¢«æ©ç çš„token æ˜¯åšå‡ ä¸ªprefill å‡ æ¬¡decode? 

ä¸»è¦æ˜¯è®¡ç®—å¿« åŒå‘æ˜¯çœŸçš„è€—èµ„æº. ä¸ºä»€ä¹ˆ è‡ªå›å½’æ¨¡å‹è®­ç»ƒæ›´ä¸ºé«˜æ•ˆ. masked modelä¸ä¹Ÿæ˜¯ä¸€æ ·çš„tokenæ•°é‡å—? 

å“¦ä»–è®­ç»ƒä¼šæ›´æ…¢,å› ä¸ºè¦ç®—æ‰€æœ‰token çš„loss. ? 

masked model , bertæ˜¯15%ï¼Œåªæœ‰15%çš„tokenå‚ä¸è®¡ç®—loss.

causal modelçš„è®­ç»ƒlabelæ€ä¹ˆè®¾ç½®.

# Lora

```
    vla_lora = PeftModel.from_pretrained(base_vla, model_id=cfg.pretrained_checkpoint, subfolder="lora_adapter") 
    vla_lora = vla_lora.merge_and_unload() 
        vla_lora = vla_lora.merge_and_unload() #eval éœ€è¦merge. è¿™æ ·å¯ä»¥è¯»å–  config, ä¸mergeä¸èƒ½è¯»å–pre ckptçš„config. model.configè¿˜æ˜¯ basevlaçš„. 
    
    
# å’Œä¸‹é¢æ˜¯ä¸ä¸€æ ·çš„ 
    # from peft import get_peft_model, LoraConfig, TaskType
    # lora_rank = 32
    # lora_config = LoraConfig(
    #             r=lora_rank,
    #             # lora_alpha=min(cfg.lora_rank, 16),
    #             lora_alpha=16,  # Xuan: usually, lora_alpha = 2 * lora_rank
    #             lora_dropout=0.0,
    #             target_modules="all-linear",
    #             init_lora_weights="gaussian",
    #         )
    # vla_lora = get_peft_model(base_vla, lora_config)
    # vla_lora.load_adapter(adapter_dir, adapter_name="default") # juyi: ä¼š assert unnorm_key in norm_stats, æŠ¥é”™.

```

https://huggingface.co/docs/peft/tutorial/peft_model_config

`mlp.up_proj.lora_A.default.weight`   

`merge_and_unload()`ä¹‹åæˆ‘ä»¬å°±ä¸æ˜¯loraæ¨¡å‹äº†, å°±æ˜¯æ™®é€šæ¨¡å‹äº†. 



