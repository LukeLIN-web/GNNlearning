







## 2-2 

https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt

å¬youtube,

padæ˜¯å› ä¸ºä¸åŒçš„batch ä¸ä¸€æ ·é•¿. attention maskä¹Ÿä¼šè®¾ä¸º0 è¡¨ç¤ºè¿™äº›æ˜¯padding çš„.

è¾“å‡ºçš„logitsä¸æ˜¯æ¦‚ç‡,  softmax ä¹‹åå˜æˆæ¦‚ç‡ 

head æ˜¯ä»€ä¹ˆ?   The model heads take the high-dimensional vector of hidden states as input and project them onto logits. å°±æ˜¯å‡ ä¸ªlinear.  å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ¶æ„æ‰§è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œä½†æ¯ä¸ªä»»åŠ¡éƒ½å°†å…·æœ‰ä¸åŒçš„ head ä¸ä¹‹å…³è”ã€‚  Transformer æ¨¡å‹çš„è¾“å‡ºç›´æ¥å‘é€åˆ°æ¨¡å‹å¤´è¿›è¡Œå¤„ç†.  çœ‹å›¾  https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg  

æ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹éƒ½è¾“å‡º logitsï¼Œå› ä¸ºç”¨äºè®­ç»ƒçš„æŸå¤±å‡½æ•°é€šå¸¸ä¼šå°†æœ€åä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ SoftMaxï¼‰ä¸å®é™…çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰èåˆ

quiz: 

- lossæ˜¯æ€ä¹ˆç®—çš„?

- logitsæ˜¯ä»€ä¹ˆ? 

  

2-3 models

config loadçš„æ—¶å€™å¯ä»¥ç›´æ¥æ”¹config. 

2-4 tokenizers 

todo : ç”¨åˆ°çš„æ—¶å€™ä»”ç»†çœ‹çœ‹

2-5 Handling multiple sequences

paddingå¿…é¡»å’Œ attention maskç»„åˆä½¿ç”¨ä¸ç„¶å°±å‡ºé”™. 

2-6tokenizers åº“

éå¸¸å¼ºå¤§, 

```
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")# å¯ä»¥è¿”å›torch, TensorFlowå’Œnumpyæ•°ç»„. 

model_inputs = tokenizer(sequence) åˆ†è¯å™¨åœ¨å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šå•è¯ [CLS]ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šå•è¯ [SEP]ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ç”¨è¿™äº›è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå› æ­¤ä¸ºäº†è·å¾—ç›¸åŒçš„æ¨ç†ç»“æœéœ€è¦æ·»åŠ . 
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence) # å’Œç›´æ¥è°ƒç”¨ä¸ä¸€æ ·!æ²¡æœ‰CLSäº†. 
ids = tokenizer.convert_tokens_to_ids(tokens) #å°±å¯ä»¥çœ‹æ•°å­—äº†.
print(tokenizer.decode(model_inputs["input_ids"])) #å°±å¯ä»¥çœ‹textäº†. ç”¨çš„æ˜¯åŒä¸€ä¸ªtokenizer


ç‰¹æ®Štoken:
llama2ç­‰æ¨¡å‹çš„ä¸­<s>æ˜¯ BOS (beginning of a sentence) token
[CLS] ä»£è¡¨åˆ†ç±»ã€‚ä¹‹æ‰€ä»¥åœ¨å¼€å¤´æ·»åŠ ï¼Œæ˜¯å› ä¸ºè¿™é‡Œçš„è®­ç»ƒä»»åŠ¡æ˜¯å¥å­åˆ†ç±»ã€‚ç”±äºä»–ä»¬éœ€è¦å¯ä»¥è¡¨ç¤ºæ•´ä¸ªå¥å­å«ä¹‰çš„è¾“å…¥ï¼Œå› æ­¤ä»–ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ ‡ç­¾ã€‚
```



3-2 

```
raw_datasets = load_dataset("glue", "mrpc") 
# todo , glueæ˜¯å•¥æ„æ€? æ²¡æ‰¾åˆ°è¿™ä¸ªåº“

raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0] #çœ‹é•¿å•¥æ ·

raw_train_dataset.features  # è¿™å°†å‘Šè¯‰æˆ‘ä»¬æ¯åˆ—çš„ç±»å‹ï¼š
label çš„ç±»å‹ä¸º ClassLabelï¼Œæ•´æ•°åˆ°æ ‡ç­¾ name çš„æ˜ å°„å­˜å‚¨åœ¨ names æ–‡ä»¶å¤¹ä¸­

tokenizerè¿˜å¯ä»¥ç”¨é€—å·ä¼ å…¥ä¸¤ä¸ª, è¿”å›token type idsæ¥è¡¨æ˜æ˜¯ç¬¬ä¸€ä¸ªè¿˜æ˜¯ç¬¬äºŒä¸ª.(è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨é€‰æ‹©å…¶ä»–æ£€æŸ¥ç‚¹ï¼Œåˆ™æ ‡è®°åŒ–è¾“å…¥ä¸­ä¸ä¸€å®šåŒ…å«token_type_idsï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨ DistilBERT æ¨¡å‹ï¼Œåˆ™ä¸ä¼šè¿”å›å®ƒä»¬ï¼‰ã€‚åªæœ‰å½“æ¨¡å‹çŸ¥é“å¦‚ä½•å¤„ç†å®ƒä»¬æ—¶ï¼Œæ‰ä¼šè¿”å›å®ƒä»¬ï¼Œå› ä¸ºå®ƒåœ¨é¢„è®­ç»ƒæœŸé—´å·²ç»çœ‹åˆ°äº†å®ƒä»¬ã€‚) è¿˜å¯ä»¥ä¼ å…¥å¤šä¸ªåºåˆ—.


ä¸ºäº†å°†æ•°æ®ä¿å­˜ä¸ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Dataset.mapï¼ˆï¼‰ æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦å®Œæˆæ›´å¤šçš„é¢„å¤„ç†è€Œä¸ä»…ä»…æ˜¯ tokenizationï¼Œè¿™ä¹Ÿä¸ºæˆ‘ä»¬æä¾›äº†ä¸€äº›é¢å¤–çš„çµæ´»æ€§ã€‚

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
è¿™å°†å…è®¸æˆ‘ä»¬åœ¨è°ƒç”¨ mapï¼ˆï¼‰ æ—¶ä½¿ç”¨é€‰é¡¹ batched=Trueï¼Œè¿™å°†å¤§å¤§åŠ å¿«åˆ†è¯é€Ÿåº¦ã€‚åˆ†è¯å™¨ç”± Tokenizers åº“ä¸­ç”¨ Rust ç¼–å†™çš„ğŸ¤—åˆ†è¯å™¨æä¾›æ”¯æŒã€‚è¿™ä¸ªåˆ†è¯å™¨å¯ä»¥éå¸¸å¿«ï¼Œä½†å‰ææ˜¯æˆ‘ä»¬ä¸€æ¬¡ç»™å®ƒå¾ˆå¤šè¾“å…¥ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨åœ¨ tokenization å‡½æ•°ä¸­çœç•¥äº† padding å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºå°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦æ•ˆç‡ä¸é«˜ï¼šæœ€å¥½åœ¨æ„å»ºæ‰¹æ¬¡æ—¶å¡«å……æ ·æœ¬ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……è¯¥æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥çš„é•¿åº¦éå¸¸å¯å˜æ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›ï¼

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# todo
ç»§ç»­é˜…è¯»åé¢å†…å®¹, Dynamic padding  
```

3-3 trainer

todo

3-4 A full training 

todo





7-6

```
å¯¹æ›´å¤šä½¿ç”¨è¿™äº›åº“çš„è®­ç»ƒæ ·æœ¬ç»™äºˆæ›´å¤šæƒé‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ pltã€pdã€skã€fit å’Œ predict ç­‰å…³é”®å­—è½»æ¾è¯†åˆ«è¿™äº›ç¤ºä¾‹ï¼Œè¿™äº›å…³é”®å­—æ˜¯ matplotlib.pyplotã€pandas å’Œ sklearn æœ€å¸¸è§çš„å¯¼å…¥åç§°ï¼Œä»¥åŠåè€…çš„ fit/predict æ¨¡å¼ã€‚å¦‚æœå®ƒä»¬éƒ½è¡¨ç¤ºä¸ºå•ä¸ª tokenï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾æ£€æŸ¥å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨ input åºåˆ—ä¸­ã€‚æ ‡è®°å¯ä»¥å…·æœ‰ç©ºæ ¼å‰ç¼€ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å°†åœ¨ tokenizer è¯æ±‡è¡¨ä¸­æ£€æŸ¥è¿™äº›ç‰ˆæœ¬ã€‚ä¸ºäº†éªŒè¯å®ƒæ˜¯å¦æœ‰æ•ˆï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªæµ‹è¯•ä»¤ç‰Œï¼Œè¯¥ä»¤ç‰Œåº”æ‹†åˆ†ä¸ºå¤šä¸ªä»¤ç‰Œï¼š

```





#### generate

å¤§éƒ¨åˆ†æ¨¡å‹ ç»§æ‰¿äº†PreTrainedModel , ç»§æ‰¿äº†GenerationMixin, è°ƒç”¨generate, å°±å¯ä»¥æŠŠself è‡ªå·±ä¼ è¿›å», è¿™é‡Œæ˜¯è°ƒç”¨äº† WhisperForConditionalGeneration ä¸­çš„forwardå‡½æ•°ã€‚è¿™æ˜¯å› ä¸º PyTorch çš„ nn.Module åŸºç±»å®šä¹‰äº†ä¸€ä¸ª __call__ æ–¹æ³•ï¼Œå½“ä½ è°ƒç”¨æ¨¡å‹å®ä¾‹ï¼ˆå³ selfï¼‰æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è°ƒç”¨è¿™ä¸ª __call__ æ–¹æ³•ï¼Œè€Œè¿™ä¸ª __call__ æ–¹æ³•åˆä¼šè°ƒç”¨ forward æ–¹æ³•ã€‚

https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0005

