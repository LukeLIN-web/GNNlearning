## Vit

Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers

HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers: 

 å¾ˆå®¹æ˜“å½•äº†HPCA, algorithm  publish åˆ«çš„paperäº†.   FPGAæ˜¯æ—é›ªçš„å­¦ç”Ÿå†™çš„, å›å›½äº†. åŠ¨æ€remove patchs, sequence layerä¹Ÿå¯ä»¥remove,  å¯ä»¥predict, è®­ç»ƒäº†ä¸€ä¸ªpredicter.  ä¼˜ç‚¹å°±æ˜¯prune ä¸€å±‚,  accä¸‹é™äº†, ä¸ä¼šä¼ æ’­.   ä¸ºä»€ä¹ˆ? é—®é—®arman .  æ²¡äººçŸ¥é“ hardwareæ€ä¹ˆæ”¯æŒä¸åŒdimensionçš„. 

Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training

ä¸€æ–‡è¯»æ‡‚Stable Diffusion è®ºæ–‡åŸç†+ä»£ç è¶…è¯¦ç»†è§£è¯» - è“è‰²ä»™å¥³çš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/640545463

Unet:  https://pic3.zhimg.com/v2-03cf776c6281ff727e157e6088dbb394_r.jpg

æ·±å…¥æµ…å‡ºå®Œæ•´è§£æStable Diffusion XLï¼ˆSDXLï¼‰æ ¸å¿ƒåŸºç¡€çŸ¥è¯† - Rocky Dingçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/643420260

ç›®å½• https://www.zhihu.com/column/c_1646154470676168704

vae : https://pic3.zhimg.com/v2-a390d53cc59c0e76b0bbc86864f226ac_r.jpg



stable diffusion training, teacheræ˜¯å†»ç»“çš„, è®­ç»ƒstudentè¿™ä¸ªvae+ Unet + encoder, ä¸¤ä¸ªloss åŠ èµ·æ¥. è¾¨åˆ«å™¨ä¼šå¹³è¡¡åˆ°50% è¾“å‡ºè¾¾åˆ°çœŸå‡éš¾è¾¨. 

åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå¦‚æœå®šä¹‰äº† 3 ä¸ªåå‘å»å™ªçš„æ­¥éª¤ï¼ˆStepï¼‰ï¼Œ**UNet ä¼šåœ¨æ¯ä¸ªæ­¥éª¤ä¸­æ‰§è¡Œä¸€æ¬¡**ã€‚æ¯ä¸€æ­¥éƒ½ä¼šå°†å½“å‰çš„å¸¦å™ªæ•°æ®ä¼ é€’ç»™ UNetï¼Œè®©å…¶å»å™ªå¹¶ç”Ÿæˆä¸€ä¸ªæ›´æ¥è¿‘æœ€ç»ˆè¾“å‡ºçš„æ•°æ®ã€‚å› æ­¤ï¼Œç»è¿‡ 3 ä¸ªæ­¥éª¤ï¼ŒUNet å°±ä¼šè¢«è°ƒç”¨ 3 æ¬¡ã€‚

#### sdxl turbo code

```
python scripts/txt2img.py --prompt "a professional photograph of an astronaut riding a horse" --ckpt /root/share/stablediffusion/checkpoints --config /root/share/stablediffusion/configs/stable-diffusion/v2-inference.yaml 

éœ€è¦numpy 1.24 é«˜äº†ä½äº†éƒ½ä¸è¡Œ.

turbo ä¸æ˜¯ckpt, ä½†æ˜¯sd çš„pytorch éœ€è¦ckpt. æ€ä¹ˆåŠ?   æˆ‘å†™äº†ä¸€ä¸ªè¯•è¯•.
ckptè¢«åºŸå¼ƒäº†, è¿™ä¸¤ä¸ªä»“åº“ä¹Ÿè¢«åºŸå¼ƒäº†, å¤§å®¶éƒ½åœ¨ç”¨diffusers.

https://github.com/diStyApps/Safe-and-Stable-Ckpt2Safetensors-Conversion-Tool-GUI ä¹Ÿä¸å¥½ç”¨. 

https://github.com/Stability-AI/generative-models/blob/main/scripts/demo/turbo.py

å¯ä»¥ç”¨è°ƒè¯• unet_debugging = torch.load("unet.pth")
ç„¶ååœ¨debugé‡Œé¢çœ‹. æˆ–è€… print (unet.xxx.attention_head_dim)  https://huggingface.co/docs/diffusers/en/api/models/unet2d-cond æ‰¾sourceæ‰¾åˆ°
https://github.com/huggingface/diffusers/blob/v0.30.3/src/diffusers/models/unets/unet_2d_condition.py#L71

debugä¹ŸæŒºå¥½çš„, å¯ä»¥çœ‹æ‰€æœ‰ä¸œè¥¿, ä¸ç”¨å†™å¾ˆå¤šprintè¯­å¥. 

è¦ä¹ˆhack diffusers. ä½†æ˜¯æœ€å¤§éš¾ç‚¹æ˜¯ä»£ç é‡å¤ªå¤§äº†, ä¸çŸ¥é“è·³è½¬åˆ°å“ªé‡Œå»äº†,å¯ä»¥debug step in. ä½†æ˜¯æ— å…³çš„ä»£ç å¤ªå¤šä¹Ÿéš¾ç ”ç©¶. 
ä½†æ˜¯ä¸è¦éšæ„ä¹±æ”¹lib,  å¯èƒ½ä»¥åå‡ºé”™äº†, ç¯å¢ƒè°ƒä¸å¯¹å°±ä¸çŸ¥é“å“ªé‡Œå‡ºé”™äº†.
æˆ‘æ‰¾åˆ°äº†optimal, å°±pip install -e .å°±è¡Œäº†. å…ˆdebugæ‰¾åˆ°ç”¨äº†å“ªä¸ªæ¨¡å‹,æ ¹æ®åå­—æ‰¾åˆ° UNet2DConditionModel. 
scripts/demo/sampling.py æ²¡æœ‰turbo, å¯ä»¥æ˜¾ç¤ºå›¾ç‰‡. 

PYTHONPATH=. streamlit run scripts/demo/turbo.py ä¸æ˜¾ç¤ºå›¾ç‰‡, ä¸ºä»€ä¹ˆ? 
è¦å…ˆç‚¹å‡»load model
open_clip_pytorch_model.binæœ‰10ä¸ªG.  
OOMäº†. 
f16å¯ä»¥ç”¨å—? æ€ä¹ˆç”¨? 
f16ä¹ŸOOM.
diffusersä¼¼ä¹é»˜è®¤ç”¨Lora å°±å¯ä»¥. 

diffusers/src/diffusers/models/unets/unet_2d_condition.py

vscode debugå¤±è´¥. attempted relative import with no known parent package
```

turboå°±æ˜¯ https://github.com/Stability-AI/generative-models 

## diffusion åŸºç¡€

https://lilianweng.github.io/posts/2021-07-11-diffusion-models/



X0->x1->x2 -> z

#### Forward diffusion process

æ¯æ¬¡åŠ ä¸€ç‚¹é«˜æ–¯å™ªå£°. 



#### reverse diffusion

ç”¨Unet, æ¯æ¬¡



## Unet

è·³è¿‡è·¯å¾„ç›´æ¥å°†ä¸°å¯Œä¸”ç›¸å¯¹æ›´å¤šçš„ä½çº§ä¿¡æ¯ä» Di è½¬å‘åˆ° Ui ã€‚åœ¨ U-Net æ¶æ„ä¸­çš„å‰å‘ä¼ æ’­æœŸé—´ï¼Œæ•°æ®åŒæ—¶é€šè¿‡ä¸¤æ¡è·¯å¾„éå†ï¼šä¸»åˆ†æ”¯å’Œ skip åˆ†æ”¯. 



skipåˆ†æ”¯ä»£ç åœ¨å“ªé‡Œ?`hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)`

è§†é‡èŒƒå›´è¶Šæ¥è¶Šå¤§, ä½†æ˜¯çœ‹ä¸åˆ°å°çš„äº†.  ä¸¤ä¸ªæ•°ä¹‹é—´ è·ç¦»å…¶å®è¶Šæ¥è¶Šå¤§. åªèƒ½çœ‹åˆ°æœ€big picture, çœ‹ä¸åˆ°ç»†èŠ‚.  æ‰€ä»¥éœ€è¦å¤§å±‚é¢çš„resnet. ç”¨concat. 

attentionå’Œ spatial transformer å·®å¼‚æ˜¯å•¥?  



#### ResnetBlock2D





#### cross attention

Q is projected from noisy data zt, K and V are projected from text condition













## pruning

#### æŒ‘æˆ˜

challenge stems from the step-by-step denoising process required during their reverse phase, limiting parallel decoding capabilities 

æ–¹æ³•:  å‡å°‘é‡‡æ ·æ­¥éª¤çš„æ•°é‡, é€šè¿‡æ¨¡å‹ä¿®å‰ªã€è’¸é¦å’Œé‡åŒ–ç­‰æ–¹æ³•å‡å°‘æ¯æ­¥çš„æ¨¡å‹æ¨ç†å¼€é”€.

éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†æ¥é‡æ–°è®­ç»ƒè¿™äº›è½»é‡çº§æ¨¡å‹.

#### DeepCache

: Accelerating Diffusion Models for Free. CVPR'24

è§‚å¯Ÿåˆ°è¿ç»­æ­¥éª¤ä¹‹é—´é«˜çº§ç‰¹å¾çš„æ˜¾ç€æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç°è¿™äº›é«˜çº§ç‰¹å¾ç”šè‡³å¯ä»¥ç¼“å­˜ï¼Œå¯ä»¥è®¡ç®—ä¸€æ¬¡ï¼Œç„¶åå†æ¬¡æ£€ç´¢ä»¥è¿›è¡Œåç»­æ­¥éª¤ã€‚é€šè¿‡åˆ©ç”¨ U-Net çš„ç»“æ„ç‰¹æ€§ï¼Œå¯ä»¥ç¼“å­˜é«˜çº§ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒåœ¨æ¯ä¸ªé™å™ªæ­¥éª¤ä¸­æ›´æ–°çš„ä½çº§ç‰¹å¾ã€‚

æ˜¯æ€ä¹ˆç¼“å­˜çš„? 

ä½œè€…æ”¹è¿›å‡ºäº†, https://github.com/VainF/Diff-Pruning

#### SnapFusion

: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds nips 23

æ²¡æœ‰å…¬å¼€ä»£ç . ä»“åº“åªæœ‰å›¾. 

pruneå’Œ NAS, éœ€è¦å¾®è°ƒ. 

fig2 è¯´ UNet ä¸­é—´éƒ¨åˆ† å‚æ•°å¤š, , the slowest parts of UNet are the input and output stages with the largest feature resolution, as spatial cross-attentions have quadratic computation complexity with respect to feature size (tokens).

 step distillationæ˜¯æ€ä¹ˆåšçš„? 

input and output stages æŒ‡çš„æ˜¯å“ªä¸ªstage?å…·æœ‰æœ€å¤§ç‰¹å¾åˆ†è¾¨ç‡ æ˜¯å“ªä¸ª? 

é€šè¿‡è¯„ä¼°å•ä¸ªæ®‹å·®å—å’Œæ³¨æ„åŠ›å—çš„é‡è¦æ€§è·å¾—é«˜æ•ˆçš„ UNet æ¶æ„. æ€ä¹ˆè¯„ä¼°? 

3.1 efficient Unet

è·³è¿‡ä¸€éƒ¨åˆ† crossattention, resnet, æ˜¯or çš„å…³ç³»è¿˜æ˜¯ä¸¤ä¸ªéšæœºç‹¬ç«‹?

algo1

```
while not conver:
    perform robust training
    if f perform architecture evolving at this iteration:
        perform architecture evolving
        for eachblock[i, j]:
            delta_clip = evaluate_block(å»æ‰i, j, )
            delta_latency = evaluate_block(i, j)
        if latency > latency_threshold:
            A = argmin(delta_clip/delta_latency)
        else:
            A = argmin(delta_clip/delta_latency)
è¿™ä¸ªç®—æ³•ä¸è®²äººè¯,éå¸¸éš¾æ‡‚. 
```

3.2 image decoder

applying 50% uniform channel pruning to the original image decoder

#### 4 Step Distillation

distilling the teacher, *e.g.*, at 32 steps, to a student that runs at fewer steps, *e.g.*, 16 steps

stepå°±æ˜¯å¾ªç¯Unet 32æ¬¡. 

ä¸è¦é€æ­¥è¿›è¡Œ, å‡­ç»éªŒè§‚å¯Ÿåˆ°ï¼Œæ¸è¿›å¼è’¸é¦æ¯”ç›´æ¥è’¸é¦ç•¥å·®. 32è’¸é¦åˆ°16, 16è’¸é¦åˆ°8 step.

16node*8ä¸ª 40G A100GPU.

#### structual pruning for for diffusion models.

https://arxiv.org/pdf/2305.10924

è¯¥æœ¯è¯­ |ğœ½â€²|0 è¡¨ç¤ºå‚æ•°çš„ L-0 èŒƒæ•°ï¼Œå®ƒè®¡ç®—éé›¶è¡Œå‘é‡çš„æ•°é‡ï¼Œå¹¶ s è¡¨ç¤ºä¿®å‰ªæ¨¡å‹çš„ç¨€ç–æ€§ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹å›ºæœ‰çš„è¿­ä»£æ€§è´¨ï¼Œè®­ç»ƒç›®æ ‡ï¼ˆç”¨ è¡¨ç¤ºï¼‰ â„’ å¯ä»¥è¢«è§†ä¸ºç›¸äº’å…³è”çš„ä»»åŠ¡çš„ç»„åˆ T ï¼š {â„’1,â„’2,â€¦,â„’T} ã€‚æ¯é¡¹ä»»åŠ¡éƒ½ä¼šå½±å“å¹¶ä¾èµ–äºå…¶ä»–ä»»åŠ¡ï¼Œä»è€Œå¸¦æ¥äº†ä¸åŒäºä¼ ç»Ÿä¿®å‰ªé—®é¢˜çš„æ–°æŒ‘æˆ˜ï¼Œä¼ ç»Ÿä¿®å‰ªé—®é¢˜ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–å•ä¸ªç›®æ ‡ä¸Šã€‚æ ¹æ®å…¬å¼ [4](https://arxiv.org/html/2305.10924?_immersive_translate_auto_translate=1#S4.E4) ä¸­å®šä¹‰çš„ä¿®å‰ªç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆæ·±å…¥ç ”ç©¶äº†æ¯ä¸ªæŸå¤±åˆ†é‡ â„’t åœ¨ä¿®å‰ªä¸­çš„å•ç‹¬è´¡çŒ®ï¼Œéšåæå‡ºäº†ä¸€ç§ä¸“ä¸ºæ‰©æ•£æ¨¡å‹ä¿®å‰ªè€Œè®¾è®¡çš„å®šåˆ¶æ–¹æ³•ï¼Œå³ Diff-Pruningã€‚

https://arxiv.org/abs/2410.10812   Hybrid Autoregressive Transformer.  ç”¨ä¼ ç»Ÿçš„ARæ¨¡å‹ç”Ÿæˆå›¾åƒ,  æ··åˆåˆ†è¯å™¨ï¼Œå®ƒå°†è‡ªåŠ¨ç¼–ç å™¨çš„è¿ç»­æ½œåœ¨å› ç´ åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šä»£è¡¨å¤§å±€çš„ç¦»æ•£åˆ†è¯å’Œä»£è¡¨ç¦»æ•£åˆ†é‡æ— æ³•è¡¨ç¤ºçš„æ®‹ä½™åˆ†é‡çš„è¿ç»­åˆ†é‡ã€‚ç¦»æ•£åˆ†é‡ç”±å¯æ‰©å±•åˆ†è¾¨ç‡çš„ç¦»æ•£ AR æ¨¡å‹å»ºæ¨¡ï¼Œè€Œè¿ç»­åˆ†é‡åˆ™ä½¿ç”¨åªæœ‰ 37M å‚æ•°çš„è½»é‡çº§æ®‹å·®æ‰©æ•£æ¨¡å—è¿›è¡Œå­¦ä¹ . 

## stable video diffusion

é‡åŒ–å¯ä»¥èŠ‚çœæ˜¾å­˜, çœGPU, pruningå¯ä»¥å—? 

34wä¸‹è½½, stable-video-diffusion-img2vid-xt, 9.56gb, fp16æ˜¯ 4.2 GB. ä½†æ˜¯è¿˜æ˜¯è·‘ä¸èµ·æ¥. ä¸æ”¯æŒbf16.

A100æ”¯æŒbf16.

ä¹Ÿå¯ä»¥çœ‹çœ‹åŠ é€Ÿ cogvideox æ™ºæ™®AIå’Œæ¸…åçš„. cogvideox æœ‰8wä¸‹è½½, éå¸¸å¯ä»¥. 

LanguageBind/Open-Sora-Plan-v1.3.0, 0ä¸‹è½½. æ²¡å•¥äººç”¨.  ä¸ç®¡ä»–.

```
torch._dynamo.exc.Unsupported: call_method NNModuleVariable() to [ConstantVariable(str)] {}

from user code:
   File "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 717, in offload
    self.hook.init_hook(self.model)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 696, in init_hook
    return module.to("cpu")

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = Truez
   Function                                  Runtimes (s)
--------------------------------------  --------------
_compile                                      146.867
OutputGraph.call_user_compiler                132.066
create_aot_dispatcher_function                132.273
compile_fx.<locals>.fw_compiler_base          113.608
```

compileçš„æ—¶é—´éå¸¸ä¹…. 

ä¸ºäº†é˜²æ­¢é‡åŒ–å¼•èµ·çš„ä»»ä½•æ•°å€¼é—®é¢˜ï¼Œæˆ‘ä»¬ä»¥ bfloat16 æ ¼å¼è¿è¡Œæ‰€æœ‰å†…å®¹ã€‚

https://pytorch.org/blog/accelerating-generative-ai-3/

torchao,FP8 precision must be used on devices with NVIDIA H100 and above

