

## LCS

**LCS是 label centric sampling的简称**

### 1摘要和简介

在概率抽样的基础上cumulatively 构建子图，并迭代训练GCN模型以生成approximate node representations

采样存在bias， 所以选一些support nodes 来最小化node aggregation的bias, 从原图中采样， round by round in a cumulative way

### 2 Related Work

GCN sampling有几类方法：

|               | examples               | pro                       | con                       |
| ------------- | ---------------------- | ------------------------- | ------------------------- |
| node-wise     | GraphSage, PinSage     | quick                     | nodes grows exponentially |
| layer-wise    | FastGCN, LADIES        | avoid recursive expansion |                           |
| Subgraph-wise | ClusterGCN, GraphSAINT |                           |                           |
| History-wise  | VRGCN  MVS-GNN         | reduce the variance       |                           |



然而， 当前的工作都没有减小输入的大小， 我们用预处理子图作为输入来减小GCN训练的成本。

Network Sampling 也有几类方法

|                          | examples             | pro  | con                       |
| :----------------------- | -------------------- | ---- | ------------------------- |
| Node sampling            | Random PageRank Node |      | nodes grows exponentially |
| Edge sampling            | Random Edge          |      |                           |
| Traversal based sampling | Random walk          |      |                           |



我们提出了一种自适应的 label-centric cumulative 网络采样方法特别适合加速GCN训练

### 3具体实现

一轮轮采样， 在第i轮， 从原图中采样一个子图， 来生成一个视图并训练。

view是逐渐变大的。也就是上面说的cumulative， 利用label-centric probabilistic 方法来减小采样的bias。

采样子图分为三步： 选择锚定节点， 选择支持节点 和建立视图。

那么怎么选择锚定节点呢？  We select the nodes which the current model is least certain with regarding to classification prediction.

提出一种概率抽样方法，用概率分布对支持节点进行抽样。

总体算法如下： 

```
Accuracy gain AG = 1, Accuracy score score = 0, Round number round = 0
while AG > theta :
	# label centric anchor nodes selection 
	randomly sample labeled nodes as candidates Da
	estimate the information entropy in Da
	select top information entropy as Ta 
	# suppport node sampling
	calculate inclusion probability 
	conduct Poisson sampling 
	# View contruction
	construct a sampled subgraph
  # GCN trian
	evaluate and get new AG
```

实验可以达到17x的加速。  本论文提出的方法理论保证了unbiased and convergent.

## **GRAND+**

### 摘要

1.  提出了一种generalized forward push (GFPush) 算法， 预先计算一个通用的传播矩阵，并采用它以小批量的方式进行图数据的增强。
2. 利用confidence-aware consistency loss， 使得模型更有泛化优势。   实验证明可以达到 SOTA accuracy. 而且有加速效果。 

### 背景

之前，为了解决过拟合问题， 提出了GRAND方法，但是GRAND 传播特征矩阵 需要 指数次迭代， 很难拓展到大型图， 所以提出了GRAND+。

1. 提出了 generalized mixedorder matrix 来进行随机特征矩阵传播
2. 引入高效的近似技术，以小批量的方式进行随机传播，解决了GRAND的可扩展性限制。
3. confidence-aware loss for the GRAND+ regularization framework

### 2.1 Problem Definition

diagonal degree matrix 有什么用？

How to make GNN scalable?

1. 节点采样
2. 图划分成多个图
3. 矩阵近似方法遵循SGC[29]的设计，将特征传播和非线性变换解耦，并利用一些近似方法来加速特征传播。

提出的GRAND+框架与基于矩阵近似的方法高度相关，比如 PPRGo [5] and GBP [8].

### 3.1

GRAND 设计了随机传播--一种混合顺序的传播策略，以实现图数据的扩增（augmentations），可以优化不同augmentations中未标记节点的预测一致性。

1. drop some feature
2. propagate with a mixed-order matrix

GRAND+的方法：

1. 预先计算传播矩阵所需的行向量，以小批量的方式进行随机传播
2. 提出了confidence-aware loss for consistency regularization，这使得训练过程更加稳定并导致更好的泛化。

## ALLIE

ALLIE: Active Learning on Large-scale Imbalanced Graphs

### 摘要

1. 使用具有不平衡感知奖励功能的强化学习agent从多数类和少数类中采样
2. 在节点分类模型中使用 focal loss 来关注很稀少的类
3. 采用图粗化策略来减少搜索空间。

### 介绍

active learning 可以动态地查询候选样本的标签， 但是对于大规模imblanaced 标签场景的探索还很少 

如何查询最 "有信息 "的样本？

正样本在标记的数据中只占很小的一部分， 而且现有的采样方法不适用于图结构数据。

目前大图的AL方法时间复杂度太高， 所以需要 减少AL算法的搜索空间。

### 贡献

Active Learning based method for Large-scale ImbalancEd graphs (ALLIE)

1. 应用强化学习策略，使分类器的性能最大化，找到未标记数据集的代表性子集
2. 采用图粗化策略，对相似节点进行分类，减少策略网络中的行动空间
3. 构建一个基于 focal loss 的分类器， down-weights the well-classified examples

### 背景

1. AL , 使用RL选择一个节点序列。现有的工作主要集中在混合数据集上。
2.  sparsification and reduction 是简化图的两种常见方式。

sparsification 包括spanners, edge cut, spctral sparsifiers.

Reduction 包括 graph coarsening and Kron reduction

\3. Imbalanced Learning的方法：　

data-level 可以用 Oversampling  and undersampling.

Algorithm-level　可以adjusting the training or inference process . Such as Focal loss　

### 具体实现 

AL  从vertex train集中查询一些节点， 策略网络是一个DNN
