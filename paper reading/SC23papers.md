## BLAD

BLAD: Adaptive Load Balanced Scheduling and Operator Overlap Pipeline For Accelerating The Dynamic GNN Training.  æ¥è‡ª sjtu ä¸æ˜¯ipadsçš„.  ä¸å¯å¤ç°. dockerhubå’Œ github éƒ½åˆ é™¤äº†.

é—®é¢˜:  high communication overhead  ->  æŠŠæ•°æ®é›†åˆ†æˆsnapshot group, ç„¶åallocates each snapshot group to different  GPU. 

long synchronization ->

and poor resource usage.  ->  å¯åŠ¨å¤šä¸ªGPU,åŒæ—¶æ‰§è¡Œcomputeå¯†é›†å’Œmemroy å¯†é›†çš„operators.  å¦å¤–, è¿˜è°ƒæ•´ modelæ‰§è¡Œé¡ºåº. 

è´¡çŒ®:  æ˜¯å¯¹åº”å“ªä¸ªé—®é¢˜

å‡è®¾:    å“ªé‡Œå¯ä»¥è¿›ä¸€æ­¥æå‡,è¢«æ”»å‡». 

è§‚å¯Ÿchallenge  :   ä¸åŒsnapshot çš„vertice æ•°é‡å·®è·å¤§.   

### æ‘˜è¦

è°ƒåº¦workload å‡å°‘åŒæ­¥å’Œé€šè®¯æ—¶é—´.  è°ƒæ•´opæ‰§è¡Œé¡ºåº.

allocates each snapshot group to a GPU

### INTRODUCTION

Vertex partition (e.g. Aligraph, DGL, PyGT) distributes the vertices of each graph snapshot to different GPUs

snapshot partition(e.g. ESDG) distributes the snapshots to different GPUs.  hidden states are transferred with the snapshot partition

é—®é¢˜: high communication overhead, long synchronization, and poor resource usage.

å› ä¸º nodeå˜åŒ–, æ‰€ä»¥ä¸åŒGPU loadä¸ balance. 

#### æ–¹æ¡ˆ

All groups **without data dependency** are scheduled to one GPU, and each GPU/node is assigned multiple groups.

æ¯ä¸ªgpuä¸¤ä¸ªè¿›ç¨‹,è®­ç»ƒä¸åŒçš„snapshot, overlapä¸åŒç±»å‹çš„op.

topology manager è°ƒæ•´ modelçš„op æ‰§è¡Œé¡ºåº. 

### 3Background

ä¹‹å‰çš„snapshotä¹Ÿè¦æä¾›feature. 

#### TWO-LEVEL LOAD SCHEDULING

 ğ‘ ğ‘”ğ‘– represents the ğ‘–-th snapshot group.

forwardä¹‹åæ”¾å…¥ queue,  ä¸ºå•¥æœ‰çš„è¿›å…¥P2, æœ‰çš„ä¸è¿›å…¥? 



## DistTGL

```bash
 https://s3.us-west-2.amazonaws.com/dgl-data/dataset/tgl/REDDIT/edge_features_e0.pt
HTTP request sent, awaiting response... 403 Forbidden
2023-12-18 12:01:25 ERROR 403: Forbidden.
# æˆ‘ç”¨è¿™ä¸ªedge featureè¯•è¯• ,åå­—ä¸ä¸€æ ·:
wget -P ./DATA/WIKI https://s3.us-west-2.amazonaws.com/dgl-data/dataset/tgl/WIKI/edge_features.pt 

python gen_minibatch.py --data WIKI --gen_eval --minibatch_parallelism 2
# gen_minibatch.py å¹²äº†ä»€ä¹ˆå‘¢? 
å¥½åƒæ˜¯æŠŠneg mfg, pos mfg éƒ½sample å‡ºæ¥å­˜èµ·æ¥.  æ˜¯å¦å®éªŒä¸å…¬å¹³? 
args.group = 0æ˜¯ä¸è¡Œçš„. 
args.group = 1 ä¼šå‡ºé”™. 
    self.tot_length = len([fn for fn in os.listdir(self.path) if fn.startswith('{}_pos'.format(mode))]) // minibatch_parallelism
FileNotFoundError: [Errno 2] No such file or directory: 'minibatches/WIKI_1_49_32/'
train_neg_samples = 1çš„æ—¶å€™,ä¸å­˜åœ¨. 
```

i j  k åº”è¯¥æ˜¯å¤šå°‘å‘¢    

 ğ‘– represents how many GPUs to compute each mini-batch,  i =2 

 ğ‘˜ represents how many copies of node memory to maintain, 

and ğ‘— represents how many epochs to train in parallel for each copy of node memory.

èƒ½ä¸èƒ½ä¸€ä¸ªæœºå™¨æ?

 https://github.com/amazon-science/disttgl  , optimize tgn multiple GPU training, memory-based TGNNs.  æå‡ºäº†ä¸‰ç§Parallelismï¼Œææ¸…æ¥šéƒ½æ˜¯ä¸ºä»€ä¹ˆåœ¨åšä»€ä¹ˆ



å“ªäº›å‡è®¾ é»˜è®¤æ˜¯å¯¹çš„? 

#### problem

1. figure3  staleness and information loss.  æ€•information leak,æ‰€ä»¥è¦æ™šä¸€ä¸ªstepè®­ç»ƒ.  è§£å†³æ–¹æ³• - > new model  æé«˜acc. 

2. need synchronous.  åŒæ­¥çš„å¼€é”€éå¸¸å¤§  ->  memory parallelismä¸ç”¨åŒæ­¥node memory.  

#### contribution:  æ€ä¹ˆè§£å†³æå‡ºçš„é—®é¢˜.

1. enhances the node memory in M-TGNNs by introducing additional static node memory,   ä¼˜åŒ–äº† accuracy and convergence rate 
2. introduces two novel parallel training strategies - epoch parallelism and memory parallelism.   è¿˜æœ‰  heuristic guidelines to determine the optimal training configurations based on the dataset and hardware characteristics.
3. adopting prefetching and pipelining techniques to minimize the mini-batch generation overhead. It serializes the memory operations on the node memory and efficiently executes them by an independent daemon process -- >  è§£å†³ complex and expensive synchronizations

### 1 ä»‹ç»

batch size å¢åŠ  ,accä¼šå‡å°‘, ä¸ºä»€ä¹ˆ? 

model: **æ·»åŠ äº†additional static node memory.**   -> accæé«˜, åŠ é€Ÿ. æ˜¯è§£å†³å“ªä¸ªé—®é¢˜? 

System:  adopting **prefetching and pipelining** techniques to minimize the mini-batch generation overhead   ->  æ˜¯è§£å†³å“ªä¸ªé—®é¢˜? 

### 2 èƒŒæ™¯

M-TGNNå¹¶è¡Œçš„ç®—æ³•:  åŸå…ˆæ˜¯process consecutive graph events that do not have overlapping nodes in batches by updating their node memory in parallel. ä½†æ˜¯è¿™ä¸ªæ–¹æ³•batch sizeä¸èƒ½å¤ªå¤§ä¸ç„¶è‚¯å®šæœ‰overlap.    ä½†æ˜¯batch sizeå¤ªå°åˆä¸èƒ½å……åˆ†åˆ©ç”¨GPUçš„å¹¶è¡Œæ€§. æ‰€ä»¥MTGNN å¤§batch å¤„ç†events,  å°‘é‡æ›´æ–° node memory.  ä½†æ˜¯è¿™ä¼šå¯¼è‡´figure3 çš„ staleness and information loss.

###  3

While this may be true on some evolving graphs like citation graphs, it fails on the dynamic graphs where  high-frequency information is important.  ä¸ºä»€ä¹ˆè¯´fails on dynamic graphs?   

ä¼˜ç‚¹:  we separate the static and dynamic node memory and capture them explicitly.  DistTGL keeps the original GRU node memory on all nodes to capture the dynamic node information and **implements an additional mechanism to capture the static node information**.  æ•ˆæœ-> æé«˜äº†acc. 

è¶…è¶Šäº†tgn. 

#### 3.2

figure 7 :  i  æ˜¯ ç¬¬iä¸ª mini batchçš„æ„æ€

mini batch, å°±æ˜¯ç®€å•çš„æ•°æ®åˆ’åˆ†. 

epoch å¹¶è¡Œ: training different epochs simultaneously using only one copy of the node memory.  æœ‰3ä¸ªGPU, åœ¨3ä¸ªiter å°±è®­ç»ƒåŒä¸€ä¸ªmini batchçš„3ä¸ªepoch, ä¼˜ç‚¹: éœ€è¦çš„å†…å­˜å°‘, åŒæ—¶å¯ä»¥capture dependency. 

ä¸ºä»€ä¹ˆéœ€è¦ negative mini-batch? 

memory parallelism: each trainer uses its own copy of the node memory to process and update the graph events within that segment.  ä¼˜ç‚¹: ä¸éœ€è¦trainerä¹‹é—´åŒæ­¥node memory

### 4

#### 4.2

DistTGL only applies memory parallelism across machines,  åªéœ€è¦åŒæ­¥weight, ä¸éœ€è¦åŒæ­¥ node memory. 



