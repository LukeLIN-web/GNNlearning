https://jeongseob.github.io/readings_mlsys.html

Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity  é˜¿é‡Œå·´å·´å’Œæ‚‰å°¼å¤§å­¦çš„VLDB24

å››ä¸ªçŸ©é˜µä¹˜æ³•, ç”¨unstructured weight pruningæ¥é™ä½å†…å­˜æ¶ˆè€—.

å…ˆæŠŠç¨€ç–çš„å˜æˆdenseçš„. For each iteration, each thread block loads ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ (shape [ğ‘€, ğ¾]) in sparse format and ğµğ‘‡ğ‘–ğ‘™ğ‘’ (shape [ğ¾, ğ‘]) in dense format from global memory. ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ is then transformed to dense format with our efficient *Sparse-to-Dense Transformation* strategy.   Finally, each thread block consumes the dense data in shared memory and generates the output tile through tensor core computations.

---

 ICML'23 BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models  åœ¨GPUä¹‹é—´ä¼ è¾“ activation. 

FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU  , å¯ä»¥æœç´¢ å­˜å‚¨å’Œè®¿é—®tensorçš„æ–¹å¼.  **GPUç«¯ä»…ä»…è¿›è¡Œä¸€ä¸ªTransformer layerçš„è®¡ç®—ï¼Œä¸€æ—¦è®¡ç®—å®Œæˆå°±å¯¹KVcacheã€æ¿€æ´»ã€weightæƒé‡å‚æ•°è¿›è¡Œcheckpointï¼Œä¹Ÿæ˜¯æµæ°´åŒ–overlappingçš„å°†æ•°æ®è½¬ç§»åˆ°CPU DRAMå’Œç£ç›˜**   æ¨ç†å»¶è¿Ÿå·²ç»æ‹‰é•¿åˆ°3.3ä¸ªå°æ—¶äº†ï¼ˆè¿™ä¹Ÿé™åˆ¶å®ƒçš„ä½¿ç”¨åœºæ™¯ï¼Œä»…é€‚åˆç¦»çº¿æ‰¹é‡è®¡ç®—åœºæ™¯ï¼‰å¯¹ç»†èŠ‚çš„æŒç»­æ€è€ƒï¼ˆæ¯”å¦‚è¿›ä¸€æ­¥é‡åŒ–å‹ç¼©æ”¹è¿›CPU-GPUè®¿å­˜å¸¦å®½ã€è®¾è®¡è‡ªåŠ¨æ–¹æ³•å¯»æ‰¾æœ€ä¼˜çš„ä¼˜åŒ–å‚æ•°ï¼‰æ¥æ”¹è¿›ç³»ç»Ÿ

GPipe å¯ä»¥è§£å†³å•å¡æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ã€‚ 

PipeDream é€šè¿‡å¿«é€Ÿåå‘ä¼ æ’­, èŠ‚çœæ˜¾å­˜, ç¼ºç‚¹æ˜¯éœ€è¦ç»´æŠ¤å¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹å‚æ•°, ä¸é€‚åˆå‚æ•°å¤šçš„LLMæ¨¡å‹. 

Megatron-LMçš„ç¬¬äºŒç¯‡è®ºæ–‡, ç»™device 1åˆ†é… å±‚1\2\9\10, è€Œä¸æ˜¯1-4å±‚, é™ä½bubble ç‡. [*Memory*-Efficient *Pipeline*-Parallel DNN Training](https://zhuanlan.zhihu.com/p/650744349)  

INFaaS: Automated Model-less Inference Serving  ,Stanford  , åŠ¨æ€åœ°é€‰æ‹©ä¸åŒå±æ€§ã€å¤§å°çš„æ¨¡å‹model-level autoscalingï¼Œåˆ©ç”¨[VM-level horizontal autoscaling]. 

[FasterTransformer](https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/FasterTransformer) by NVIDIA  å±‚ä¼˜åŒ–ï¼šèåˆè¿›å•ä¸€kernelï¼›activation cacheï¼›é‡ç”¨æ¯ä¸€å±‚activate/outputçš„å†…å­˜bufferï¼›tensorå¹¶è¡Œå’Œpipelineå¹¶è¡Œã€é€šä¿¡ä¼˜åŒ–ï¼›MatMulåº•å±‚å®ç°æ–¹å¼è‡ªåŠ¨è°ƒæ•´ï¼›

STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining, ASPLOS 23, UVA  fleix xiaozhu lin.   æ¨¡å‹åˆ†ç‰‡ã€‚STI å°†æ¨¡å‹å‚æ•°ä½œä¸ºç‹¬ç«‹å¯è°ƒçš„åˆ†ç‰‡è¿›è¡Œç®¡ç†ï¼Œå¹¶åˆ†æå®ƒä»¬å¯¹å‡†ç¡®æ€§çš„é‡è¦æ€§ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨é¢„åŠ è½½ç¼“å†²åŒºè¿›è¡Œå¼¹æ€§ç®¡é“è§„åˆ’ã€‚STI å®ä¾‹åŒ– IO/è®¡ç®—ç®¡é“ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªå°ç¼“å†²åŒºè¿›è¡Œé¢„åŠ è½½åˆ†ç‰‡ï¼Œä»¥å¼•å¯¼æ‰§è¡Œï¼Œè€Œä¸ä¼šåœ¨æ—©æœŸé˜¶æ®µåœæ»;å®ƒæ ¹æ®åˆ†ç‰‡å¯¹èµ„æºå¼¹æ€§æ‰§è¡Œçš„é‡è¦æ€§æ˜æ™ºåœ°é€‰æ‹©ã€è°ƒæ•´å’Œç»„è£…åˆ†ç‰‡ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚   we implement the decompression in separate 200 SLOC of C code using OpenMP

Serving DNNs like Clockwork: Performance Predictability from the Bottom Up, osdi 2020,   online global policy. 

 MnnFast: a fast and scalable system architecture for memory-augmented neural networks ä¸ºäº†å‡å°‘å†…å­˜å¸¦å®½æ¶ˆè€—ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºåˆ—çš„æµå¼ç®—æ³•ï¼Œè¯¥ç®—æ³•æœ€å¤§é™åº¦åœ°å‡å°‘äº†æ•°æ®æº¢å‡ºçš„å¤§å°ï¼Œå¹¶éšè—äº†å¤§éƒ¨åˆ†ç‰‡å¤–å†…å­˜è®¿é—®å¼€é”€ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é™ä½é«˜æ˜‚çš„è®¡ç®—å¼€é”€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶è·³è·ƒä¼˜åŒ–æ¥ç»•è¿‡å¤§é‡çš„è¾“å‡ºè®¡ç®—ã€‚æœ€åï¼Œä¸ºäº†æ¶ˆé™¤ç¼“å­˜äº‰ç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºé«˜æ•ˆç¼“å­˜åµŒå…¥çŸ©é˜µçš„åµŒå…¥ç¼“å­˜ åœ¨FPGAä¸Š. 

æœ‰ä¸¤ç§,  forward inference-based (inf-based) approach and backend update based (upd-based) approach. æ¥æ›´æ–°dynamic graphs.

inf base, åªåœ¨æ”¶åˆ°çš„æ—¶å€™æ”¹å˜å›¾ç»“æ„. aligraph.

https://github.com/zheng-yp/DecoupledDGNN

æ²¡æœ‰ä»£ç :

1. Efficient Scaling of Dynamic Graph Neural Networks. SC'21

2. SPEED: Streaming Partition and Parallel Acceleration for Temporal Interaction Graph Embedding

3. Redundancy-Free High-Performance Dynamic GNN Training with Hierarchical Pipeline Parallelism

4. Cache-based gnn system for dynamic graphs
5. STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs https://arxiv.org/pdf/2309.15875.pdf
6. DynaGraph: Dynamic Graph Neural Networks at Scale
7. Approximate Caching for Efficiently Serving Diffusion Models 

#### ink stream 

InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update . 

 https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9644829  ä¹Ÿæ˜¯åŠ é€Ÿgnn inference.  ç¼“å­˜åˆ‡åˆ†vericesã€‚ é€‰æ‹©æœ€ä¼˜çš„inference ç®—æ³•

åŠ¨æ€å›¾çš„inferenceæœ‰ä¸€äº›paper æ¯”å¦‚inkstream ï¼Œå®é™…åº”ç”¨å’ŒMotivationæ›´å¼º

https://arxiv.org/pdf/2309.11071.pdf

é—®é¢˜: æ”¹äº†ä¹‹åå†fetch neighbor å†…å­˜ä¸å¤Ÿ.

æ–¹æ³•: å¯ä»¥Incremental Updateã€‚ æ€ä¹ˆincremental updateï¼Ÿ  åªfetch å—å½±å“çš„nodeã€‚  å¿«äº†300xï¼Œ æ„Ÿè§‰èƒ½ä¸­é¡¶ä¼šã€‚

tgnn benchmarkã€‚

#### ä»‹ç»

figure1 è¯´æ˜subgraph construction å æ®äº†50%.

figure3 è¯´æ˜å—å½±å“çš„åªæœ‰1%. ä½†æ˜¯åªç®— affected area ä¹Ÿè¦å‡ ç§’.

https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9644829  PCGraph. ä¹Ÿæ˜¯åŠ é€Ÿgnn inference.

survey : https://arxiv.org/pdf/2306.14052.pdf   . A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware

#### workloadè°ƒæŸ¥

å¤§éƒ¨åˆ†éƒ½æ˜¯ä»ç¡¬ä»¶åŠ é€Ÿçš„è§’åº¦æ¥çš„.  éƒ½æ˜¯é™æ€åˆ°è¾¾, ä¸æ˜¯æ…¢æ…¢åˆ°è¾¾, æ²¡æœ‰ä»è¿›å…¥patternæ¥è€ƒè™‘.  

1. FlowGNN: A Dataflow Architecture for Real-Time Workload-Agnostic Graph Neural Network Inference.  gatech,  æ˜¯ç”¨message ,embedding, FPGAæ¥åŠ é€Ÿ featureåŠ è½½. è§’åº¦ä¸åŒ.    ç”¨çš„æ˜¯æ™®é€šçš„dataset.
2. å¾ˆå¤šéƒ½æ˜¯ç”¨Channel Pruning .  accelerate GNN inference by pruning the dimensions in each layer with negligible accuracy loss. viktor@USCçš„å·¥ä½œ.  è¿˜æœ‰è£å‰ª feature .
3. GNNIE: æ•°æ®é›†å°±æ˜¯æ™®é€šçš„, ä¸æ˜¯åŠ¨æ€åˆ°è¾¾çš„. 
4. Aligraph , It provides a performance guarantee of sampling P99 latency in 20ms on large-scale dynamic graphs.   å‡ ç™¾ä¸ªworker, åœ¨æ·˜å®æ•°æ®é›†ä¸Š,  ä»–ä»¬æœ‰å¾ˆå¤šæ¨¡å‹, æµ‹äº†æ¨¡å‹ç²¾åº¦.
5. Automatic Generation of High-Performance Inference Kernels for Graph Neural Networks on Multi-Core Systems æ˜¯ç¼–è¯‘å™¨ä¼˜åŒ–
6. Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU . åˆ†æäº†é—®é¢˜
7. **DGI: Easy and Efficient Inference for GNNs** ä¹Ÿå°±æ˜¯é™æ€çš„æ•°æ®é›†. ä»–æå‡ºäº†å¿«é€Ÿç¿»è¯‘ä»£ç åˆ°layer wise.å’Œæˆ‘ä»¬æ˜¯æ­£äº¤çš„. 
8. SERVING GRAPH COMPRESSION FOR GRAPH NEURAL NETWORKS ä¹Ÿæ˜¯é™æ€çš„æ•°æ®é›†. ä»–ä»¬è¯æ˜acc lossä¸å¤§. 

#### GNN serving in a cluster 

å¥½åƒæ²¡æœ‰è®¨è®ºè¿‡

quiveræœ‰ cluster, ä¸‰ä¸ªserver,  their scalability becomes limited by these network bottlenecks.

quiver latency å°±æ˜¯æµ‹ sample +  to device + forwardçš„æ—¶é—´. thoughtput  å°±æ˜¯batch size / æœ€åçš„end time- ç¬¬ä¸€ä¸ªend time.

### serving+é€Ÿçš„æ–¹æ³•

å¦‚æœä¸´æ—¶è¾“å‡ºè¾ƒå¤§å°±å¯ä»¥ç®—å­èåˆã€‚ å¯¹äºdense layer, å¯ä»¥å †å batch å¤„ç†.

