https://jeongseob.github.io/readings_mlsys.html

Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity  é˜¿é‡Œå·´å·´å’Œæ‚‰å°¼å¤§å­¦çš„VLDB24

å››ä¸ªçŸ©é˜µä¹˜æ³•, ç”¨unstructured weight pruningæ¥é™ä½å†…å­˜æ¶ˆè€—.

å…ˆæŠŠç¨€ç–çš„å˜æˆdenseçš„. For each iteration, each thread block loads ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ (shape [ğ‘€, ğ¾]) in sparse format and ğµğ‘‡ğ‘–ğ‘™ğ‘’ (shape [ğ¾, ğ‘]) in dense format from global memory. ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ is then transformed to dense format with our efficient *Sparse-to-Dense Transformation* strategy.   Finally, each thread block consumes the dense data in shared memory and generates the output tile through tensor core computations.

---

SHEPHERD: Serving DNNs in the Wild , nsdi 2023 , ä¸€ä½œå¼ å¼˜. å»¶è¿Ÿè¦æ±‚: 50-500ms

NSDI 2023æœ‰å“ªäº›å€¼å¾—å…³æ³¨çš„æ–‡ç« ï¼Ÿ - å­™æŒºSuntçš„å›ç­” - çŸ¥ä¹
https://www.zhihu.com/question/543376768/answer/3119939466

clock work osdi 2020,   online global policy. 

nexus , å‘¨æœŸæ€§çš„, per-stream policy.

calculat CV for each group streaming, 100-1000ä¸ªstream, å°±ä¼šæ¯”è¾ƒç¨³å®šè€Œä¸”å¯ä»¥é¢„æµ‹åˆ°è¾¾pattern.

æ€ä¹ˆgroupå‘¢? ç­‰å—?   åœ¨æ¯ä¸ªgroup ä¸­ online serving. 

æœ‰ä¸¤ç§,  forward inference-based (inf-based) approach and backend update based (upd-based) approach. æ¥æ›´æ–°dynamic graphs.

inf base, åªåœ¨æ”¶åˆ°çš„æ—¶å€™æ”¹å˜å›¾ç»“æ„. aligraph.

https://github.com/zheng-yp/DecoupledDGNN

æ²¡æœ‰ä»£ç :

1. Efficient Scaling of Dynamic Graph Neural Networks. SC'21

2. SPEED: Streaming Partition and Parallel Acceleration for Temporal Interaction Graph Embedding

3. Redundancy-Free High-Performance Dynamic GNN Training with Hierarchical Pipeline Parallelism

4. Cache-based gnn system for dynamic graphs
5. STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs https://arxiv.org/pdf/2309.15875.pdf
6. DynaGraph: Dynamic Graph Neural Networks at Scale
7. Approximate Caching for Efficiently Serving Diffusion Models 

#### ink stream 

InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update . 

 https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9644829  ä¹Ÿæ˜¯åŠ é€Ÿgnn inference.  ç¼“å­˜åˆ‡åˆ†vericesã€‚ é€‰æ‹©æœ€ä¼˜çš„inference ç®—æ³•

åŠ¨æ€å›¾çš„inferenceæœ‰ä¸€äº›paper æ¯”å¦‚inkstream ï¼Œå®é™…åº”ç”¨å’ŒMotivationæ›´å¼º

https://arxiv.org/pdf/2309.11071.pdf

é—®é¢˜: æ”¹äº†ä¹‹åå†fetch neighbor å†…å­˜ä¸å¤Ÿ.

æ–¹æ³•: å¯ä»¥Incremental Updateã€‚ æ€ä¹ˆincremental updateï¼Ÿ  åªfetch å—å½±å“çš„nodeã€‚  å¿«äº†300xï¼Œ æ„Ÿè§‰èƒ½ä¸­é¡¶ä¼šã€‚ 

tgnn benchmarkã€‚

#### ä»‹ç»

figure1 è¯´æ˜subgraph construction å æ®äº†50%.

figure3 è¯´æ˜å—å½±å“çš„åªæœ‰1%. ä½†æ˜¯åªç®— affected area ä¹Ÿè¦å‡ ç§’.

https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9644829  PCGraph. ä¹Ÿæ˜¯åŠ é€Ÿgnn inference.

survey : https://arxiv.org/pdf/2306.14052.pdf   . A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware

#### workloadè°ƒæŸ¥

å¤§éƒ¨åˆ†éƒ½æ˜¯ä»ç¡¬ä»¶åŠ é€Ÿçš„è§’åº¦æ¥çš„.  éƒ½æ˜¯é™æ€åˆ°è¾¾, ä¸æ˜¯æ…¢æ…¢åˆ°è¾¾, æ²¡æœ‰ä»è¿›å…¥patternæ¥è€ƒè™‘.  

1. FlowGNN: A Dataflow Architecture for Real-Time Workload-Agnostic Graph Neural Network Inference.  gatech,  æ˜¯ç”¨message ,embedding, FPGAæ¥åŠ é€Ÿ featureåŠ è½½. è§’åº¦ä¸åŒ.    ç”¨çš„æ˜¯æ™®é€šçš„dataset.
2. å¾ˆå¤šéƒ½æ˜¯ç”¨Channel Pruning .  accelerate GNN inference by pruning the dimensions in each layer with negligible accuracy loss. viktor@USCçš„å·¥ä½œ.  è¿˜æœ‰è£å‰ª feature .
3. GNNIE: æ•°æ®é›†å°±æ˜¯æ™®é€šçš„, ä¸æ˜¯åŠ¨æ€åˆ°è¾¾çš„. 
4. Aligraph , It provides a performance guarantee of sampling P99 latency in 20ms on large-scale dynamic graphs.   å‡ ç™¾ä¸ªworker, åœ¨æ·˜å®æ•°æ®é›†ä¸Š,  ä»–ä»¬æœ‰å¾ˆå¤šæ¨¡å‹, æµ‹äº†æ¨¡å‹ç²¾åº¦.
5. Automatic Generation of High-Performance Inference Kernels for Graph Neural Networks on Multi-Core Systems æ˜¯ç¼–è¯‘å™¨ä¼˜åŒ–
6. Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU . åˆ†æäº†é—®é¢˜
7. **DGI: Easy and Efficient Inference for GNNs** ä¹Ÿå°±æ˜¯é™æ€çš„æ•°æ®é›†. ä»–æå‡ºäº†å¿«é€Ÿç¿»è¯‘ä»£ç åˆ°layer wise.å’Œæˆ‘ä»¬æ˜¯æ­£äº¤çš„. 
8. SERVING GRAPH COMPRESSION FOR GRAPH NEURAL NETWORKS ä¹Ÿæ˜¯é™æ€çš„æ•°æ®é›†. ä»–ä»¬è¯æ˜acc lossä¸å¤§. 

#### GNN serving in a cluster 

å¥½åƒæ²¡æœ‰è®¨è®ºè¿‡

quiveræœ‰ cluster, ä¸‰ä¸ªserver,  their scalability becomes limited by these network bottlenecks.

quiver latency å°±æ˜¯æµ‹ sample +  to device + forwardçš„æ—¶é—´. thoughtput  å°±æ˜¯batch size / æœ€åçš„end time- ç¬¬ä¸€ä¸ªend time.

### serving+é€Ÿçš„æ–¹æ³•

å¦‚æœä¸´æ—¶è¾“å‡ºè¾ƒå¤§å°±å¯ä»¥ç®—å­èåˆã€‚ å¯¹äºdense layer, å¯ä»¥å †å batch å¤„ç†.

