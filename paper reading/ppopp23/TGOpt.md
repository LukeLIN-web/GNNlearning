# TGOpt: Redundancy-Aware Optimizations for Temporal Graph Attention Networks

Charith Mendis  æ•™æˆ, ä¸»è¦æ˜¯åš ai  ç¼–è¯‘çš„. ä½œè€…uiuc mscs ç›´æ¥å»ç‰¹æ–¯æ‹‰å·¥ä½œäº†, è¿˜æäº†TGLite, ä½†æ˜¯å¥½åƒæ²¡ä¸­. 

é—®é¢˜

1.  inference æ”¾åœ¨training è¿˜workå—ï¼Ÿ è®ºæ–‡åªæœ‰inference, æ²¡æœ‰training, ä¸ºä»€ä¹ˆ?     å› ä¸ºè®­ç»ƒçš„æ—¶å€™model parameters and weights ä¼š change. æ‰€ä»¥embeddingå°±ä¸ä¸€æ ·.  inferenceå¯ä»¥å­˜å‚¨embedding
2. semantic-preservingä»€ä¹ˆæ„æ€? 

## æ‘˜è¦

åŠ é€Ÿinference, proposes to accelerate TGNN inference by de-duplication, memorization, and pre-computation.

## 1 introduction

We observe and leverage redundancies in temporal embedding and time-encoding computations to considerably reduce TGAT inference run- time.

ä¼˜ç‚¹:  the redundancy elimination techniques we consider are not restricted to simple aggrega- tions and do not require replacing self-attention.

- Processing source and destination nodes in batches can result in redundant node embedding calculations
- Calculating embeddings for a node and timestamp may result in recalculations of the same embeddings due to exploring the same temporal neighborhoods
- For some dynamic graphs, up to 89.9% of the total embeddings generated during their lifetime can be repeated calculations
- The time-encoding operation in TGAT is frequently invoked with the same time delta values

## 2 Background

TGAT dataset ,  They store the edge linkages, edge features and node features respectively. https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs

å¯ä»¥å¤„ç†èŠ‚ç‚¹åˆ†ç±»å’Œ link prediction. å½’çº³æ¨æ–­æ–°èŠ‚ç‚¹å’Œè§‚å¯Ÿåˆ°çš„èŠ‚ç‚¹çš„åµŒå…¥

Layer å’Œé™æ€çš„ä»€ä¹ˆåŒºåˆ«? 

mj(t) = msg(å‡ºèŠ‚ç‚¹çš„è€h ,  å…¥èŠ‚ç‚¹çš„è€h, è¾¹çš„ç‰¹å¾)  æ„Ÿè§‰ä¹Ÿå·®ä¸å¤š? å°±æ˜¯è¦å¤šè¿­ä»£å‡ æ¬¡time.  

ri å°±æ˜¯ä¸€æ ·, æŠŠmsg ç”¨summation ç­‰å‡½æ•° æ¥aggr, hiä¹Ÿæ˜¯ä¸€æ ·, å åŠ NN.   hå°±æ˜¯temporal embeddings.

æ˜¯å¯¹äºæ¯ä¸ªeij éƒ½è¦åšä¸€æ¬¡gnnæ“ä½œå—? 

RandEdgeSampler, æœ‰ä»€ä¹ˆç”¨?  å°±æ˜¯éšæœºæ‰¾å‡ ä¸ªè¾¹ä½œä¸ºbackgroundå¯¹æ¯”. 

It learns a function Î¦   that maps a time value to a ğ‘‘ğ‘¡ -dimensional vector. This time-encoding technique allows it to capture temporal patterns of the graph. The time-encoding vector è¾“å…¥ the input features of a GNN operator, thereby incorporated into the output embeddings.

temporal neighborhood :  tj >t

å‡è®¾ node featureä¸å˜

## 3 Redundancies & Reuse Opportunities

### 3.1 Duplication From Batched Edges

nodes often share common neighbors and this can lead to duplicate âŸ¨ğ‘–, ğ‘¡ âŸ© pairs.   æ¯”å¦‚åŒä¸€æ—¶åˆ» a->c , b->c. é‚£ä¹ˆå°±æœ‰ä¸¤ä¸ª<c,t>  

### 3.2Temporally Redundant Embedding Calculations

3 most-recent neighbors  å¯èƒ½åªå˜åŒ–äº†ä¸€ä¸ª, **model parameters and weights do not change** during inference time, æ‰€ä»¥ä¼šæœ‰ç›¸åŒè¾“å‡º.  

### 3.3 Repeated Time Encodings

 Eqs. (4, 5),   it always uses 0 for the time-encoding of ğ‘§ğ‘– (ğ‘¡). Performing this computation every time is unnecessaryâ€”since **the weights for the time-encoder** are fixed at inference time

å¤§éƒ¨åˆ† time deltaséƒ½æ¥è¿‘0 . 

ä¼˜åŒ–:  we can compute this once ahead-of-time and reuse it indefinitely.

## 4 Redundancy-Aware Optimizations

### 4.1Deduplicating Nodes

 jointly operates on the two separate arrays in order to avoid creating intermediate tensors.  hashç®—æ³•, äº§ç”Ÿunique element , 

### 4.2 Memoization of Embeddings

ä¸ºä»€ä¹ˆè¦ComputeKeys?  æ˜¯æ ¹æ®keyæ¥æ‰¾é‡å¤çš„.  The inputs will generally be combined into a single key value by hashing. the ComputeKeys operation can be performed in parallel across the pairs, æ˜¯å¯ä»¥å¹¶è¡Œçš„

ä¹Ÿæ˜¯pull å’Œpush embedding. 

the list of neighbors, their edge timestamps, edge features, and ğ» (ğ‘™ âˆ’1) as inputs.

combined into a single key value by hash

Storage Memory Location: åœ¨CPU. æ‰€ä»¥è¦ move ğ» ğ‘š to CPU device;?

We also note that each of the keys can be operated on independently, so the main loop in both CacheStore and CacheLookup can be parallelized, given that TGOpt uses a concurrent hash table implementation. We selectively paral- lelize these operations depending on the hardware.  

`    #ifndef tgopt_1t_cache_lookup`  

### 4.3 Precomputing Time Encodings

precomputes time-encoding vectors in advance before running inference.

ç›´æ¥apply the Î¦(Â·) function ,è¿™ä¸ªå‡½æ•°æ˜¯ time-encoding function , learns a function Î¦  that maps a time value to a ğ‘‘ğ‘¡ -dimensional vector. This time-encoding technique allows it to capture temporal patterns of the graph.

ä¼˜åŒ–äº†lookup è¿‡ç¨‹.

ä»€ä¹ˆæ˜¯ time-encoding vectors? å°±æ˜¯æŠŠæ—¶é—´ä¹Ÿç¼–ç ä½œä¸ºä¸€ä¸ªå˜é‡. 

## 5 å®éªŒ

speedups of 4.9Ã— on CPU and 2.9Ã— on GPU

baseline ç”¨TGL.

table5 è¯æ˜ GPUæ¬è¿ embeddingå¾ˆèŠ±æ—¶é—´, æ‰€ä»¥ å­˜åœ¨CPU.   figure5æˆ‘æˆåŠŸå¤ç°äº†.



## 6Related Work

ä¹‹å‰æœ‰äººprecomputing a time-encoding lookup table, which is hardcoded to 128 intervals

ç¼ºç‚¹: the self-attention in TGNNs was replaced with a simplified version, thereby altering the semantics

æ²¡æœ‰ä¼˜åŒ–redundancy optimizations.

#### dynaGraph 

å­˜ä¸­é—´embedding, åªæ”¯æŒDTDG.   æ‰€ä»¥åšCTDG.

#### HAG abstraction. 

ä¸æ”¯æŒå¤æ‚çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶. 

ReGNN is more tailored to hardware accelerators

## 7 future work

1. è®­ç»ƒåŠ é€Ÿ

2. ç›®å‰æ˜¯most-recent neighbor sampling strategy. å¯ä»¥å°è¯•ä¸åŒçš„sampling æ–¹æ³•. 

3. å½“node feature changes and deletion of edges, æ€ä¹ˆåŠ? 

å¾ˆå¤štuition æ¥è‡ªref42 , FPGA.

tensorcoreèƒ½ç”¨åœ¨è¿™é‡Œå—,  å¯ä»¥åŒæ—¶è®¡ç®—16*16 *16 çš„ä¸‰ç»´çš„.

A100æ”¯æŒblock is sparse. 

## ä»£ç 

https://github.com/ADAPT-uiuc/tgopt

#### ç¯å¢ƒ

Docker file å†™çš„æ˜¯ ` pip install torch==1.12.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu`

ä¸ºä»€ä¹ˆè£…cpuç‰ˆæœ¬?  è®ºæ–‡è¯´åœ¨cpuä¸Šæœ‰å“è¶Šçš„æ€§èƒ½. å› ä¸ºembeddingä½äºCPU. 

300è¡Œä»£ç ä¸€ä¸ªcppæ–‡ä»¶å°±æå®šäº†. ä»£ç é‡å°. 

è¿˜æ˜¯å¾—è£…gpu,  å› ä¸ºGPUæ›´å¿«, è®ºæ–‡ç”¨çš„æ˜¯cuda11.6,  Nvidia GPU (we tested on Tesla V100 16GB via an AWS instance). 

ç”¨torch11.6gpuçš„image

```
é”™è¯¯: 
tgopt_ext.cpp:1:10: fatal error: tbb/concurrent_unordered_map.h: No such file or directory
 #include <tbb/concurrent_unordered_map.h>
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
éœ€è¦ sudo apt-get install libtbb
```

#### æ•°æ®

Model tgat, è®ºæ–‡å…¶å®è®­ç»ƒäº†tgatçš„æ¨¡å‹



trainæ—¶é—´

| dataset    | size                                                         | cpu(s) | 1gpu(s) |
| ---------- | ------------------------------------------------------------ | ------ | ------- |
| jodie-wiki | 533M  INFO:root:num of instances: 157474.  INFO:root:num of batches: 788 | 89     | 11.3    |
| jodie-mooc | 39.5M INFO:root:num of instances: 411749.  INFO:root:num of batches: 2059 | 33     | 22      |
| snap-email | 1.6M INFO:root:num of instances: 332334.  INFO:root:num of batches: 1662 | 85     | 22      |
| snap-msg   | 337K INFO:root:num of instances: 59835. INFO:root:num of batches: 300 | 15     | 4,3.5   |



inference  old node. 

| dataset    | size                             | 1gpu(s) | 1gpu(s)  optimize |
| ---------- | -------------------------------- | ------- | ----------------- |
| jodie-wiki | 533M  , num_test_instance: 23621 | 22.6,   | 19.0              |
| jodie-mooc | 39.5M num_test_instance: 61763   | 37.3    | 34.8              |
| snap-email | 1.6M                             | 41      | 33.8              |
| snap-msg   | 337K                             | 8.4     | 8.49 (old)        |



æå‡æ²¡æœ‰å¥½å‡ å€. 

snap-msg,  old nodeæ²¡æœ‰åŠ é€Ÿ, new nodeåŠ é€Ÿäº†40%, ä¸ºä»€ä¹ˆ? 

```
./data-download.sh  snap-email jodie-mooc
python data-reformat.py -d  snap-email  snap-msg  å°±æ˜¯æŠŠsnapçš„æ–‡ä»¶è½¬æ¢ä¸ºjodie æ ¼å¼. 
python data-process.py -d jodie-wiki ä¹Ÿæ˜¯æ•°æ®å¯¹é½. 
python inference.py -d snap-email  --model tgat --prefix test --opt-all 
python inference.py -d snap-msg --model tgat --gpu 0
python train.py -d snap-msg --model tgat --prefix test --opt-all --gpu 0
python  e2einference.py -d snap-msg  --model tgat  --gpu 0
py-spy record -o profile.svg -- python benchmark/benchmarktgat.py .py -d jodie-wiki 

python benchmark/benchmarktgat.py -d snap-msg  --model tgat  --gpu 0

nsys profile -w true -t cuda,nvtx,cudnn,cublas --cuda-memory-usage=true --force-overwrite true -x true python benchmark/benchmarktgat.py -d snap-msg
```

è®ºæ–‡é‡Œè¯´30ç§’å°±inferå®Œæˆäº†. ä½†æ˜¯æˆ‘æµ‹130s 88s ,  ç”¨äº† 7ä¸ªCPU, vscode serever/htopè¦å æ®ä¸€ä¸ªcpu.

dedup_src_ts æ˜¯ä»€ä¹ˆç”¨? 

val for new nodes å’Œval for old node æ˜¯å•¥æ„æ€?  trainæ—¶è§è¿‡çš„å°±æ˜¯old, newå°±æ˜¯æ²¡æœ‰trainçš„. 

a unified framework for tgnn framework

pos_score å’Œ  neg_score ç›¸åŠ ä¸ºä»€ä¹ˆä¸ä¸º1ï¼Ÿ   å› ä¸ºscoreä¸æ˜¯ probabilities. poså’Œnegæ˜¯ç‹¬ç«‹çš„. 

è¿™ä¸ªforwardå’Œcontrastæœ‰ä»€ä¹ˆåŒºåˆ«? contrastæœ‰å¯¹äºBackgroundçš„å¯¹æ¯”. å¤ªå¥‡æ€ªäº†, è¿™ä¸ªforwardå¥½åƒæ²¡æœ‰ç”¨åˆ°, tgopt. 

ç¼ºç‚¹: ä»–ä¸æ˜¯end to end çš„.

