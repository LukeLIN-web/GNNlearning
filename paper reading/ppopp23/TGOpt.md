Charith Mendis  æ•™æˆ, ä¸»è¦æ˜¯åš ai  ç¼–è¯‘çš„.

é—®é¢˜

1.  inference æ”¾åœ¨training è¿˜workå—ï¼Ÿ è®ºæ–‡åªæœ‰inference, æ²¡æœ‰training, ä¸ºä»€ä¹ˆ?     model parameters and weights ä¼š change. æ‰€ä»¥embeddingå°±ä¸ä¸€æ ·. 
2. semantic-preservingä»€ä¹ˆæ„æ€? 

## æ‘˜è¦



## 1 introduction

We observe and leverage redundancies in temporal embedding and time-encoding computations to considerably reduce TGAT inference run- time.

ä¼˜ç‚¹:  the redundancy elimination techniques we consider are not restricted to simple aggrega- tions and do not require replacing self-attention.

- Processing source and destination nodes in batches can result in redundant node embedding calculations
- Calculating embeddings for a node and timestamp may result in recalculations of the same embeddings due to exploring the same temporal neighborhoods
- For some dynamic graphs, up to 89.9% of the total embeddings generated during their lifetime can be repeated calculations
- The time-encoding operation in TGAT is frequently invoked with the same time delta values

## 2 Background

It learns a function Î¦   that maps a time value to a ğ‘‘ğ‘¡ -dimensional vector. This time-encoding technique allows it to capture temporal patterns of the graph. The time-encoding vector è¾“å…¥ the input features of a GNN operator, thereby incorporated into the output embeddings.

temporal neighborhood :  tj >t

å‡è®¾ node featureä¸å˜

## 3 Redundancies & Reuse Opportunities

### 3.1 Duplication From Batched Edges

nodes often share common neighbors and this can lead to duplicate âŸ¨ğ‘–, ğ‘¡ âŸ© pairs.   æ¯”å¦‚åŒä¸€æ—¶åˆ» a->c , b->c. é‚£ä¹ˆå°±æœ‰ä¸¤ä¸ª<c,t>

### 3.2Temporally Redundant Embedding Calculations

3 most-recent neighbors  å¯èƒ½åªå˜åŒ–äº†ä¸€ä¸ª, **model parameters and weights do not change** during inference time, æ‰€ä»¥ä¼šæœ‰ç›¸åŒè¾“å‡º.  

### 3.3 Repeated Time Encodings

 Eqs. (4, 5),   it always uses 0 for the time-encoding of ğ‘§ğ‘– (ğ‘¡). Performing this computation every time is unnecessaryâ€”since **the weights for the time-encoder** are fixed at inference time

å¤§éƒ¨åˆ† time deltaséƒ½æ¥è¿‘0 . 

ä¼˜åŒ–:  we can compute this once ahead-of-time and reuse it indefinitely.

## 4 Redundancy-Aware Optimizations

### 4.1Deduplicating Nodes

 jointly operates on the two separate arrays in order to avoid creating intermediate tensors.  hashç®—æ³•, äº§ç”Ÿunique element , 

### 4.2 Memoization of Embeddings

ä¹Ÿæ˜¯pull å’Œpush embedding. 

the list of neighbors, their edge timestamps, edge features, and ğ» (ğ‘™ âˆ’1) as inputs.

combined into a single key value by hash

Storage Memory Location: åœ¨CPU

### 4.3 Precomputing Time Encodings

precomputes time-encoding vectors in advance before running inference.

ç›´æ¥apply the Î¦(Â·) function ,è¿™ä¸ªå‡½æ•°æ˜¯ time-encoding function , learns a function Î¦  that maps a time value to a ğ‘‘ğ‘¡ -dimensional vector. This time-encoding technique allows it to capture temporal patterns of the graph.

ä¼˜åŒ–äº†lookup è¿‡ç¨‹.

ä»€ä¹ˆæ˜¯ time-encoding vectors? 

## 5 å®éªŒ

speedups of 4.9Ã— on CPU and 2.9Ã— on GPU

baselineæˆ‘ä»¬å¯ä»¥ç”¨TGL.

## 6Related Work

ä¹‹å‰æœ‰äººprecomputing a time-encoding lookup table, which is hardcoded to 128 intervals

ç¼ºç‚¹: the self-attention in TGNNs was replaced with a simplified version, thereby altering the semantics

æ²¡æœ‰ä¼˜åŒ–redundancy optimizations.

#### dynaGraph 

å­˜ä¸­é—´embedding, åªæ”¯æŒDTDG. æˆ‘ä»¬å¯ä»¥åšCTDG

#### HAG abstraction. 

ä¸æ”¯æŒå¤æ‚çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶. 

ReGNN is more tailored to hardware accelerators

## 7 future work

1. è®­ç»ƒåŠ é€Ÿ

2. ç›®å‰æ˜¯most-recent neighbor sampling strategy. å¯ä»¥å°è¯•ä¸åŒçš„sampling æ–¹æ³•. 

3. å½“node feature changes and deletion of edges, æ€ä¹ˆåŠ? 

å¾ˆå¤štuition æ¥è‡ªref42 , FPGA.

tensorcoreèƒ½ç”¨åœ¨è¿™é‡Œå—,  å¯ä»¥åŒæ—¶è®¡ç®—16*16 *16 çš„ä¸‰ç»´çš„.

A100æ”¯æŒblock is sparse. 

### ä»£ç 

https://github.com/ADAPT-uiuc/tgopt

#### ç¯å¢ƒ

` pip install torch==1.12.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu`

ä¸ºä»€ä¹ˆè£…cpuç‰ˆæœ¬. 

300è¡Œä»£ç ä¸€ä¸ªcppæ–‡ä»¶å°±æå®šäº†. 





#### dedup_src_ts



