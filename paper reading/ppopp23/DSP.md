DSP: Efficient GNN Training with Multiple GPUs

Cuhk James Chenç»„ å’Œäºšé©¬é€Š

DSP adopts a tailored data layout to utilize the fast NVLink connections among the GPUs

For efficient graph sampling with multiple GPUs, we introduce a collective sampling primitive (CSP)

design a producer-consumer-based pipeline,

The speedup of DSP can be up to 26x and is over 2x in most cases.

Quiver and DGL-UVA cannot fully utilize the GPUs as they execute the kernels for different tasks sequentially.

æ–°layoutæ˜¯ä»€ä¹ˆ? 

## Introduction

è§£å†³çš„é—®é¢˜: the amount of read data is larger than requested 

figure1 , CSPä¸ºå•¥æ¯”idealè¿˜ä½?  paperé‡Œé¢è§£é‡Šäº†ï¼Œå› ä¸ºæœ‰ä¸€äº›æ•°æ®å·²ç»Cacheäº†

CSP generally expresses different graph sampling schemes and is efficient by using fast NVLinks and pushing tasks to data.

æ–°CSPæ˜¯ä»€ä¹ˆ? 

## 3 DSP

ç”¨METIS

DSP partitions the graph topology into patches and stores one patch on each GPU. è¿™ä¸ªå…¶å®æˆ‘ä¹Ÿå‘ç°äº†,å¯ä»¥å……åˆ†åˆ©ç”¨locality.  ideaæœç„¶è¿˜æ˜¯è¦è‡ªå·±æƒ³. 

For the node feature vectors, we cache as many hot nodes as possible in GPU memory. 

DSP uses large in-degrees to select hot nodes by default and is compatible with other criteria.

å’Œquiverä¸åŒ, DSP uses a partitioned cache, where each GPU caches different feature vectors. éœ€è¦çš„æ—¶å€™å®ƒä¼šç”¨all-to-all NVlinkæ¥ä¼ è¾“. 

### 3.2 Training Procedure

åŒæ­¥ç”¨BSP, bulk synchronous parallel, an iteration should observe all model updates made in its preceding iterations.

ä¸ºå•¥loaderèƒ½åœ¨GPUä¸Š? GPUèƒ½å‘èµ·ä¼ è¾“å—? å¯ä»¥çš„, CUDA UVA å…è®¸ç”¨æˆ·åˆ›å»ºè¶…å‡º GPUå†…å­˜å¤§å°çš„æ•°æ®ï¼ŒåŒæ—¶åˆ©ç”¨ GPU å†…æ ¸è¿›è¡Œå¿«é€Ÿè®¡ç®—ã€‚å°†æ•´ä¸ªå›¾ç»“æ„åŠå…¶ç‰¹å¾å­˜å‚¨åœ¨UVAåï¼Œå¯é€šè¿‡GPUå†…æ ¸å¿«é€Ÿæå–å­å›¾ç‰¹å¾.

ä¸ä»…hotå¯ä»¥nvlinkæ¯”pcieæ›´å¿«,æ›´é‡è¦çš„æ˜¯ hotå’Œcold å¯ä»¥å¹¶è¡Œ. 

nvlinkç”¨NCCL, nvshmem åº“å¯èƒ½æ›´å¿«, ä½†æ˜¯å¯èƒ½æœ‰äº›GPU serveræ²¡æœ‰nvlinkç”¨ä¸äº†.  (æ„Ÿè§‰è¿™æ˜¯ä¸ªå¾ˆæ‰¯æ·¡çš„åŸå› )

å¤šæœºçš„æ—¶å€™, å¤åˆ¶hot feature.  åˆ†å‰²cold feature.

## 4 **Collective Sampling Primitive**

sample åˆ†ä¸ºä¸‰æ­¥.ä»”ç»†çœ‹figure5

For shuffle, each frontier node is transferred to the GPU holding its adjacency list; 

for sample, each GPU locally samples the required number of neighbors for the frontier nodes it receives;  

reshuffle, the sampled neighbors of each frontier node ğ‘£ is transferred back to the GPU requesting neighbor samples for node ğ‘£.

ä¸ºä»€ä¹ˆéƒ½è¦ä¼ è¾“? ä¸ä¼šå¾ˆæ…¢å—?   

æ¯ä¸ªstageéƒ½æœ‰åŒæ­¥, å¼‚æ­¥åè€Œå¾ˆæ…¢. ä¸ºä»€ä¹ˆ? the communication and sampling tasks of a single GPU are small. æ˜¯ä»€ä¹ˆæ„æ€? 

CSP æ˜¯åŒæ­¥çš„, synchronization barrier at the end of each stage.

we assign each seed node to the GPU holding its adjacency list and manage a well-connected graph patch with one GPU, which makes many accesses to the adjacency lists local on each GPU.  nvlink å……åˆ†åˆ©ç”¨, å‡å°‘PCIeä¼ è¾“.

### 4.2

ç»Ÿä¸€äº†layer-wiseå’Œ node-wise.

## 5 Pipeline for GNN Training

ä¸ºäº†æé«˜GPUåˆ©ç”¨ç‡, ç”¨pipeline. 

the communication kernels of the sampler åªéœ€è¦å¾ˆå°‘çš„çº¿ç¨‹å°±fully utilize the NVLink bandwidth.

ä»–ä»¬å‘ç°setting the queue capacity limit to 2 is sufficient for overlapping the tasks.

#### é˜²æ­¢æ­»é”

éœ€è¦ä¸€ä¸ªGPUä½œä¸ºleader, æŒ‡å®šorder.

On each GPU, a worker registers its id in a pending set

the queue to manage its ready communication kernels and start them in their submission order.

DSP åšäº†ä¸‰ä¸ªstage pipeline. 

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ä»–ä»¬çš„pipelineæ€ä¹ˆå†™, ç„¶åæ”¹è¿›.   ä»–ä»¬çš„GPU ä½¿ç”¨ç‡éå¸¸é«˜. figure6 , ä¸ºä»€ä¹ˆ?ä»–ä»¬æ€ä¹ˆè¾¾åˆ°è¿™ä¹ˆé«˜çš„. ä»–ä»¬åšçš„å¾ˆgeneral, ä¸æ˜¯ä¸“é—¨çš„ä¼˜åŒ–.



## 6 implement

The graph patch on each GPU is stored using the compressed sparse row (CSR) format

Each GPU holds an adjacency position list and a feature position list

## 7å®éªŒ

æ•°æ®é›†: Papers,1äº¿èŠ‚ç‚¹,   Friendster 66Mä¸ªèŠ‚ç‚¹

We do not compare with GNNLab [42] as it requires the graph topology to fit in one GPU and conducts asynchronous training for efficiency.

 we train a 3-layer Graph- SAGE model with a hidden size of 256 for all layers and a batch size of 1024 and conduct neighbor-wise sampling with a fan-out of [15, 10, 5] by default

DSPå¿«é€Ÿæ”¶æ•› å› ä¸ºhas a shorter mini-batch time.

## ä»£ç 

https://zenodo.org/record/7463498#.ZAXVhuxBxjs

githubä¸Šæ²¡æœ‰è¿™ç¯‡æ–‡ç« çš„ä»£ç .

```
docker pull zhouqihui/dsp-ppopp-ae:latest
docker run --rm -it --runtime=nvidia --ipc=host --network=host zhouqihui/dsp-ppopp-ae:latest /bin/bash
```



### æ•°æ®é›†

The first is, Why DSP_AE-master/pyg/friendster.py has "assert False", it is correct?

```
  def process(self):
	assert False
```

The second is, I am using zhouqihui/dsp-ppopp-ae:latest `conda activate pyg`. But it doesn't have installed Pytorch. 

ç”¨DSPçš„ mpsç‰ˆæœ¬.  åœ¨ä¸åŒmodelä¸Šçœ‹çœ‹å·®è·.

ä»–åªæœ‰graphsage. 



é—®é¢˜

`FileNotFoundError: [Errno 2] No such file or directory: '/data/ds/distdgl/ogb-product1/ogb-product.json'`



#### dgl ds

ä»–ä»¬å†™äº† ParallelNodeDataLoader

ç”¨torch.cuda.stream æ¥æé«˜å¹¶è¡Œåº¦. 

```
class SubtensorLoader(Thread):
	self.out_pc_queue = MPMCQueue(1, loader_number, 1)

```



