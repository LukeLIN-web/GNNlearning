这几篇比较经典而且有意思.  

- [**Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads**](https://web.cs.ucla.edu/~harryxu/papers/dorylus-osdi21.pdf)**.**
-  [**GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs**](https://www.usenix.org/system/files/osdi21-wang-yuke.pdf) 
- [**Marius: Learning Massive Graph Embeddings on a Single Machine**](https://www.usenix.org/system/files/osdi21-mohoney.pdf) 
- [**P3 : Distributed Deep Graph Learning at Scale.**](https://www.usenix.org/system/files/osdi21-gandhi.pdf)

## **P3**

### **摘要**

基于流水线并行push-pull 的执行策略, 与一个简单的缓存策略相结合

### 简介

#### 问题

1. 网络通信产生具有特征的嵌入计算图
2. 旧的图切分方法不好用 
3. GPU未被充分利用

#### 解决方法

1. GNN 模型参数少，但是数据样本多 =》移动特征向量对网络带宽有要求 =》只把图结构分到多个机器上.  
2. 只 pull 图结构,  从不通过网络传输特征, 而是将计算最密集层（第1层）的计算图结构push 到所有机器上
3. pull 更小的 partial activations

#### 主要贡献

基于混合并行性的执行策略，将层内模型的并行性和数据并行性

#### 缺点

当特征维度较大时，会产生严重的通信开销

### 3  P3 Pipelined Push-Pull

#### 3.1 Independent Hash Partitioning Graph & Features

#### 3.2 Push-Pull Parallelism

#### 3.3 Pipelining

staleness.this delay is fixed and bound to three.是怎么代码实现的呢? 

## Marius

### 摘要

分区缓存和缓冲区感知的数据排序，以尽量减少磁盘访问和交错插入数据移动与计算，以最大限度地提高利用率.  Marius被设计用来减少数据移动的开销。

- 流水线训练和IO
- 分区缓存和缓冲感知的数据排序

我们通过以下方式扩展GNN

- 针对邻居采样和GNN聚合优化数据结构
- core外的 GNN训练

### 介绍

问题⇒方法

1. 全局嵌入是计算和内存密集型的，有IO限制。
2. 在数据移动过程中，GPU的利用率很低。

because DGL-KE dot product → data transfer  (自己要看一下为什么)

PBG. embedding parameters partitions from disk storage →  GPU.    当分区被交换时，会导致GPU的利用率不足。

1. how to ⬆️ utilization?  asynchronous training of nodes with *bounded staleness*. update to the embedding vectors for nodes are sparse  → (why? 这个可能要看第二段) asynchronous training -》 CPU memory  (why? 这个要看abozing paper )

更新边的嵌入向量是 dense的  → 需要同步训练-》在GPU内存中进行

(why a few edge types need a dense update?)

store embedding parameters on disk.  design an in-memory *partition buffer,buffer-aware* ordering 在磁盘上存储嵌入参数。 设计一个内存中的分区缓冲器，缓冲器感知的排序

#### Contribution

1. *Buffer-aware Edge Traversal Algorithm (BETA)*, 一种生成最少IO数据排序的算法
2. 将*BETA*排序与分区缓冲器和通过流水线的异步IO相结合

## Preliminaries

### 2.1

inputs: graphs with multiple edge types, such as knowledge graphs.

Each edge e = (s, r, d)  is defined as a triplet containing a source node, **relation**, and destination node.

The relation is also represented by an *embedding*.

embedding models rely on *score functions* that capture the structural properties of the graph

CPU 存embedding 参数：  DGL-KE （GPU利用率太低）， GraphVite。  

利用块存储，  PyTorch BigGraph (PBG)  缺点： partition swap 分区很贵。GPU也会idle。

#### Data Movement Challenges

### 3 Pipelined Training Architecture

五级流水线 （图4）四个阶段负责数据移动操作，一个阶段负责模型计算和GPU内参数更新。四个数据移动阶段有可配置的工作线程数量，而模型计算阶段只使用一个工作线程，以确保存储在GPU上的embedding被同步更新。

算法1

```python
for i in range(batch num):
	Bi = getBatchEdges(i);
  tn = getCpuParameters(Bi );
  cudaMemCpy(Bi , tn );# transferBatchToDevice
  tr = getGpuParameters(Bi );
  Batch = formBatch(Bi , tn , tr );
  Gn , Gr = computeGradients(Batch );
  updateGpuParameters(Bi , Gr );
  transferGradientsToHost(Gn);
  updateCpuParameters(Bi , Gn );
```

stage1 load edge ,  and the corresponding node embedding

stage2 移动到GPU

stage3 计算， 异步更新edge的embedding

Stage4 移动回cpu

stage5 更新 node embedding

为什么这样快呢？ 我猜因为cpu和gpu可以并行工作？ 

#### 缺点

流水线中， 可能存在多份相同的embedding。叫做stale version

解决方法：  限制同一时间进入流水线的的batch数量。

### 4 内存外训练

利用分块buffer， 可以隐藏IO的等待时间。

需要找到一个ordering， 来最小化交换分块的次数。 也就是Buffer-aware Edge Traversal Algorithm BETA 算法。 

1. 找到最少需要swap的次数
2. 找到最少次数swap的顺序

### 5 实验

证明了单GPU的时候比 DGL-KE和PBG训练epoch 数更少。 Marius实现了相同的精度，但比现有系统快一个数量级。Marius可以在单个AWS P3.2xLarge实例上扩展到具有超过10亿条边和高达550GB模型参数的图形实例。

### 6 讨论

对于dense或者sparse的图， 需要不同的存储和GPU并行方式。

### 7结论

介绍了Marius，一个在单机上计算大规模图嵌入模型的新框架。为了优化数据移动并最大限度地提高GPU的利用率，提出了一个流水线架构，利用分区缓存和BETA排序。在未来，我们计划探索如何将Marius设计背后的想法和我们新的数据排序应用于分布式设置，并帮助加快图神经网络的训练。

### 我们可以学到什么

1. GNN 特性对内存管理的影响

## **GNNAdvisor**

GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs

### 摘要

1. performance-relevant features from both the GNN model and the input graph(需要具体去看)
2. 提出2D工作负载管理
3. 利用GPU的内存层次结构来加速

### 简介

问题⇒方法

1.之前一些优化都是一刀切，没有优化多样化的GNN
2.不解决GNN和图处理之间的差异=》 需要调用更积极的并行性和更有效的内存优化。 (为什么？)
3.只通过编译器或手动优化的库进行静态优化 ⇒ 运行时间 ，节点度和嵌入大小

torch作为前端

Loader&Extractor找到输入信息，然后进行优化

decider可以自动设置运行时参数。

**Kernel&Runtime Crafter Only 1 GPU**

贡献:

1. 探索GNN的输入属性
2. 在GPU上进行新颖的2D工作负载管理（§4）和专门的内存定制（§5）。分析建模和参数自动选择（§6）

#### 输入分析

和GNN update 阶段比较固定的计算不同， aggregation 阶段是比较多样的。 主要分为两种

1  aggregation 邻居节点的嵌入， 比如GCN。

2  aggregation 边的特征，比如GIN

### runtime优化

runtime优化: neighbor grouping, 均衡负载, 利用内存locality. 

GNNAdvisor (Wang et al., 2021)  utilize neighbor grouping to balance the workloads among GPU threads and blocks and exploit memory locality. 

GNNAdvisor further use Rabbit Reordering (Arai et al., 2016) to maximize the graph mod- ularity by clustering. By neighbor grouping and graph reordering, the runtime workload balance and memory locality are improved by introducing some preprocessing overhead.

## Dorylus

Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads

这篇也是分布式GNN, 和dist DGL 区别是什么?

serverless 计算, 低成本高拓展性.  这个是关注了什么?

### 摘要

计算分离=》构建一个deep and bounded的异步流水线，其中图和张量并行任务可以完全重叠。

### 简介

问题: 

1. CPU的内存足够大，但没有并行性
2. 每一个epoch必须重复采样，时间开销⬆️，精度⬇️，收敛不能保证。
3. Lambda有网络延时

假设: 

1. 并非GNN训练中的所有操作都需要Lambda的并行性.
2. Lambda与GNN的张量计算完美契合(他说因为CPU贵,是么? )

方法

1. graph-parallel (CPU instance) neighbor propagations (*e.g.*, Gather and Scatter) over the input graph.  data gathering from neighbor vertices 在输入的graph上进行graph并行（CPU实例）的邻居传播（例如，gather和scatter） 从邻居顶点收集数据。
2. tensor-parallel ( Lambdas) per-vertex/edge NN operations (such as Apply)  *parameter updates*  张量并行（Lambdas）vertex/edge NN操作（如Apply）*参数更新*。
3. network latency = 》   BPAC model, *bounded pipeline asynchronous computation,  fine grained task*
4. asynchrony slow down the convergence?  *bounds the degree of asynchrony*

### Contribution

1. use Lambdas , cheap
2. pipelining each fine-grain computation operation in GCN training over numerous CPU threads  scalable solution on very large graph

sources:

1. video https://www.usenix.org/conference/osdi21/presentation/thorpe

GNN  has 4 stage

scatter  send data to neighbor, may send over network.  memory intensive

apply edge。apply NN to edge data.   compute intensive

gather,  agg func

和 apply vertex  , 再apply NN to aggregated data

async solution

1. sync before new epoch.  →S staleness bound,  cannot get S epochs ahead of others
2. sync after every scatter. 依赖neighboneighborr的feature   →cache parameter

### Drawback

suffers from the communication bottleneck
