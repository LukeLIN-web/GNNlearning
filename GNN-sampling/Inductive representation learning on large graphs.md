

知乎上有很多解读了. 

GraphSAGE：我寻思GCN也没我牛逼 - 蝈蝈的文章 - 知乎 https://zhuanlan.zhihu.com/p/74242097

在图神经网络中，节点的特征对于最后的输出（标签分类等）至关重要，因此有许多文章着重于节点特征的生成，比如SkipGram和图卷积网络。但这些方法大都无法处理未见过的新节点.

文章提出了一种可以通过归纳框架(inductive framework)完成节点Embedding计算的方法，也就是GraphSage算法，通过该算法可以对不可见点完成计算。GraphSage算法中embedding的计算步骤主要分为两步：**(1)采样(sampling)**，**(2)聚合(aggregating)**。后面的算法原理与实现也就是围绕着这两步开展的。

采样:Sample neighborhood 

首先从目标节点出发，向下逐层搜索找到一定数量的子节点，直到达到我们所设定的搜索深度后停止，那么一路上走过的子节点就构成了我们从目标节点完成的采样点

聚合/前向传播: Aggregate feature information)从最底层开始，逐层向上计算父节点的节点信息，直到目标节点，这就有了一层一层向前传播的感觉

在目标节点上计算生成其label向量，这个向量即体现了节点自身的特征信息，也学习到了局部领域内图结构的信息

引言和之前的文章[图卷积网络](https://zhuanlan.zhihu.com/p/424259073)，[图注意力网络](https://zhuanlan.zhihu.com/p/424650459)一样，这篇文章也是图神经网络领域的经典之作：进一步探索了节点的特征提取。在图神经网络中，节点的特征对于最后的输出（标签分类等）至关重要，因此有许多文章着重于节点特征的生成，比如SkipGram和图卷积网络。但这些方法大都无法处理未见过的新节点：以SkipGram为例，如果有一个新的节点加入，则需要重复整个特征生成的过程来将新节点纳入考虑 （更多关于SkipGram/DeepWalk可以参考[这篇文章](https://link.zhihu.com/?target=https%3A//medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72)）。**GraphSAGE的主要贡献就是能够直接生成未见过的节点的特征（embedding）。**

目录引言方法

transductive和inductive learning的区别

损失函数和AGGREGATE函数未知点的特征生成

实验结果

最后References

#### **方法**

在正式的讲解GraphSAGE前，我们需要理解**transductive和inductive learning的区别**：

总的来说，在图神经网络的领域中：transductive指的是在训练过程中，我们已经得到了完整的图，包括有标签的训练节点和无标签的需要预测的节点。相关方法：DeepWalk, GCN

而inductive指的是训练过程中只有可训练的节点，因此可以应对不断变化的图，比如社交网络。相关方法：GraphSAGE

1. 对于已知的点，如何进行训练？
2. 对于未知的点，如何将它和已知点联系起来？

对于已知的点，如何进行训练？

训练的第一个重点在于，我们希望训练得到的特征中，相邻点的特征相似，距离较远的点特征相异。这可以通过损失函数来表达

已知点的训练的第二个重点在于，我们希望定义并训练得到一个AGGREGATE（后面简称A）函数，这个函数的用处在于整合周围环境中的节点特征得到当前点的特征。考虑到节点的非序性，这个函数必须是对称的，即输入数据的排序不影响输出（可以参考[PointNet](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1612.00593.pdf)中对于非序性的考量）。此文中讨论了多种对称的函数：平均，最大化（pooling），和LSTM（输入随机排序的数据来模拟非序性）。不同函数的区别将在实验结果处展示。

#### 基本思想

假设我们要**聚合K次**，则需要有K个**聚合函数（aggregator）**，可以认为是N层。 每一次聚合，都是把上一层得到的各个node的特征聚合一次，在假设该node自己在上一层的特征，得到该层的特征。如此反复聚合K次，得到该node最后的特征。

在上面的过程中，我们需要学习各个聚合函数的参数，因此需要设计一个损失函数。 损失函数是设计是根据目标任务来的，可以是无监督的，也可以是有监督的。

#### 邻居的定义：

前面一直都没讨论一个点，那就是如何选择一个节点的邻居以及多远的邻居。

这里作者的做法是设置一个**定值**，每次选择邻居的时候就是**从周围的直接邻居（一阶邻居）**中均匀地采样固定个数个邻居。作者发现，**K不必取很大的值，当K=2时，效果就灰常好了**，也就是只用扩展到2阶邻居即可。至于邻居的个数，文中提到S1×S2<=500，即两次扩展的邻居数之积小于500，大约**每次只需要扩展20来个邻居即可**

GCN 不用很深, 2-3层就效果很好了.

#### Aggregator Architectures(卷积形式)

不同于习惯在图像等信息上运算的卷积核，此处的卷积运算需要能够在无序向量上运行，从而使神经网络可以训练并应用于任意顺序的节点邻域特征集。文章尝试了三种不同的卷积(聚合)核——

**均匀化卷积核(Mean aggregator)**    

**LSTM卷积核(LSTM aggregator)**   

LSTM具备更强的表达能力，但是不具备扰动不变性，作者使用加扰动的邻居节点后生成的无序节点集合来操作LSTM。

  **池化卷积核(Pooling aggregator)**   

平均池化和最大化池化，可以起到发掘样本深层信息的作用，同时又具备训练参数较少的特点。文章这里提出的是一种最大化池，对当前节点 ![[公式]](https://www.zhihu.com/equation?tex=v) 的每一个邻居节点 ![[公式]](https://www.zhihu.com/equation?tex=u_%7Bi%7D) 传入定义的卷积核中，经激活函数后按element-wise做最大化判断。

文中指出在所做实验中，均匀化卷积核与最大化池化卷积核没有明显的区别，因此剩余的实验全部全部使用的后者。

对于未知的点，如何将它和已知点联系起来？

大概流程就是：

1. （行4）通过已经训练好的AGGREGATE来提取未知点周围的信息，得到这个未知点的特征
2. （行5）将当前点的特征和周围点的特征合并，然后非线性的输出新特征
3. （行2-8）重复以上步骤K次

这里的K可以控制‘周围环境’的尺寸。比如当K=1时，意味着只考虑和未知点直接相邻的邻接点，而K越大，则以为考虑更多间接相邻的节点。

### 实验设置与模型参数(Experimental set-up)

文章设计了四组模型与GraphSAGE做比较：(1)随机分类(random classifer)；(2)忽略图结构的逻辑回归(logistic regression feature-based classifier)；(3)DeepWalk算法；(4)考虑原始特征(raw features)的DeepWalk embedding。

所有的GraphSAGE模型都设置层数 ![[公式]](https://www.zhihu.com/equation?tex=K%3D2) ，每层的规模为 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B1%7D%3D25) 和 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B2%7D%3D20) 。

除去DeepWalk，所有模型的训练都是在Tensorflow上开展的，优化器选择Adam Optimizer。

可以看出GraphSAGE相比其他几种方法，在三个数据集上的F1值都是更高的，更多的比较则是不同卷积核的选择，文章中说GraphSAGE-mean和GraphSAGE-pool效果没什么差别，但这个表格看似乎最大池化操作更好一些；另外，LSTM在Reddit数据上表现好于其他模型和其在其他数据集上的表现，可以认为用户浏览评论的过程本身就与长短期记忆的作用相吻合，因此效果较好。

### 训练时长和灵敏度分析

A图是各个算法的训练时长，不同卷积核之间差别不大，但明显DeepWalk时间更长，这是因为对不可见节点的RandomWalk和SGD优化需要了大量的时间完成。

图B是邻居节点数量对F1值和运算时间的影响，可以看到当采样点大于30个以后，F1值的增长变得缓慢，同时运算时间迅速增长，因此文章建议设置层数 ![[公式]](https://www.zhihu.com/equation?tex=K%3D2) ，每层的规模为 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B1%7D%3D25) 和 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B2%7D%3D20) 。

## Conclusion

指出合并有向图、在多模态图上应用、非均匀邻居采样函数是可以进一步改进和探索的方向。此文也是图神经网络领域的经典，引入了新的inductive特征学习的考量，取得了很好的结果。虽然文章中已经提出并比较了三种不同的AGGREGATE函数，但若能将对于AGGREGATE函数的选择一并纳入到训练过程，相信会更好的作用于实际应用。