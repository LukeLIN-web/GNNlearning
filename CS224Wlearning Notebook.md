

###   lec1

#### ä»»åŠ¡

â€¢èŠ‚ç‚¹åˆ†ç±»: é¢„æµ‹ä¸€ä¸ªèŠ‚ç‚¹çš„å±æ€§. ä¾‹å¦‚ ç»™ç”¨æˆ·å’Œå•†å“åˆ†ç±» ,AlphaFold

â€¢è¾¹é¢„æµ‹: é¢„æµ‹æ˜¯ä¸æ˜¯æœ‰è¾¹. ä¾‹å¦‚ çŸ¥è¯†å›¾è°±è¡¥å…¨, å¥½å‹å…³ç³»åˆ†æ,æ¨èç³»ç»Ÿ

â€¢å›¾åˆ†ç±»:  ä¾‹å¦‚åˆ¤æ–­åˆ†å­æ˜¯å“ªç§åˆ†å­

â€¢èšç±»(clustering): åˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦å¯ä»¥æ„æˆå­å›¾ ä¾‹å¦‚ ç¤¾äº¤åœˆåˆ¤æ–­

â€¢å›¾ç”Ÿæˆ: å¯»æ‰¾æ–°è¯ç‰©

â€¢å›¾æ¼”åŒ–: ç‰©ç†æ¨¡æ‹Ÿ 

### lec2 ä¼ ç»Ÿç‰¹å¾

#### node level

##### centrality

ï‚¡ Node centrality ğ‘ğ‘£ takes the node importance in a graph into account

Eigenvector centrality: the sum of the centrality of neighbouring nodes:

ï‚¡ Betweenness centrality: â–ª A node is important if it lies on many shortest paths between other nodes. æœ€çŸ­è·¯ç»è¿‡å®ƒçš„æ•°é‡

ï‚¡ Closeness centrality: â–ª A node is important if it has small shortest path lengths to all other nodes  åˆ«çš„èŠ‚ç‚¹åˆ°ä½ çš„è·ç¦»åŠ èµ·æ¥æœ€çŸ­. 

 Clustering coefficient counts #(triangles) that a node touches.ä¸‰è§’å½¢è¶Šå¤š è¿™ä¸ªç³»æ•°è¶Šå¤§.

GDV counts #(graphlets) that a node touches.  Graphlet degree vectorçš„æ„ä¹‰åœ¨ä¸å®ƒæä¾›äº†å¯¹äºä¸€ä¸ªèŠ‚ç‚¹çš„æœ¬åœ°ç½‘ç»œæ‹“æ‰‘çš„åº¦é‡ï¼Œè¿™æ ·å¯ä»¥æ¯”è¾ƒä¸¤ä¸ªèŠ‚ç‚¹çš„GDVæ¥åº¦é‡å®ƒä»¬çš„ç›¸ä¼¼åº¦ã€‚ç”±äºGraphletçš„æ•°é‡éšç€èŠ‚ç‚¹çš„å¢åŠ å¯ä»¥å¾ˆå¿«å˜å¾—éå¸¸å¤§ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¼šé€‰æ‹©2-5ä¸ªèŠ‚ç‚¹çš„Graphletæ¥æ ‡è¯†ä¸€ä¸ªèŠ‚ç‚¹çš„GDVã€‚

è·å¾—èŠ‚ç‚¹featureçš„æ–¹æ³•æœ‰:

â–ª Importance-based features: 

1 Node degree 2  Different node centrality measures 

â–ª Structure-based features: 

1 Node degree 2 Clustering coefficient 3 Graphlet count vector

#### link-level

â–ª Distance-based feature 

â–ª local/global neighborhood overlap

#### graph level

Kernel methods are widely-used for traditional ML for graph-level prediction.

æœ‰ä¸¤ç§kernel :  Graphlet Kernel [1]  å’Œ Weisfeiler-Lehman Kernel [2]

##### Graphlet Kernel

ç›®æ ‡: Design graph feature vector ğœ™  

æ ¸å¿ƒæ€æƒ³: Count the number of different graphlets in a graph.

é—®é¢˜: if ğº and ğº â€² have different sizes, that will greatly skewååº¦(ç­‰äºdistort) the value. 

ï‚¡ è§£å†³æ–¹æ³•: normalize each feature vector

ç¼ºç‚¹: Counting graphlets is expensive! å¦‚æœæšä¸¾çš„è¯æ˜¯ NP hardçš„,  æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´æœ‰æ•ˆç‡çš„graph kernel

##### Weisfeiler-Lehman Kernel

ç›®æ ‡: è®¾è®¡é«˜æ•ˆçš„graph feature vector ğœ™  

æ ¸å¿ƒæ€æƒ³: Use neighbourhood structure to iteratively enrich node vocabulary.

Color refinement ç®—æ³•, ä¸€å¼€å§‹æ¯ä¸ªnodeä¸€ä¸ªé¢œè‰², åé¢è¿­ä»£refine. After ğ¾ steps of color refinement å°±å¯ä»¥ summarizes the structure of ğ¾-hop é‚»å±…

ï‚¡ In total, time complexity is linear in #(edges).  å’Œè¾¹çš„æ•°é‡æˆçº¿æ€§å…³ç³»

Graph is represented as Bag-of-colors

### lec3 node embed

å›¾åµŒå…¥ï¼ˆGraph Embeddingï¼‰ï¼Œå±äºè¡¨ç¤ºå­¦ä¹ çš„èŒƒç•´ï¼Œä¹Ÿå¯ä»¥å«åšç½‘ç»œåµŒå…¥ï¼Œå›¾è¡¨ç¤ºå­¦ä¹ ï¼Œç½‘ç»œè¡¨ç¤ºå­¦ä¹ ç­‰ç­‰ã€‚é€šå¸¸æœ‰ä¸¤ä¸ªå±‚æ¬¡çš„å«ä¹‰:

- **å°†å›¾ä¸­çš„èŠ‚ç‚¹è¡¨ç¤ºæˆä½ç»´ã€å®å€¼ã€ç¨ å¯†çš„å‘é‡å½¢å¼**ï¼Œä½¿å¾—å¾—åˆ°çš„å‘é‡å½¢å¼å¯ä»¥åœ¨å‘é‡ç©ºé—´ä¸­å…·æœ‰è¡¨ç¤ºä»¥åŠæ¨ç†çš„èƒ½åŠ›ï¼Œè¿™æ ·çš„å‘é‡å¯ä»¥ç”¨äºä¸‹æ¸¸çš„å…·ä½“ä»»åŠ¡ä¸­ã€‚ä¾‹å¦‚ç”¨æˆ·ç¤¾äº¤ç½‘ç»œå¾—åˆ°èŠ‚ç‚¹è¡¨ç¤ºå°±æ˜¯æ¯ä¸ªç”¨æˆ·çš„è¡¨ç¤ºå‘é‡ï¼Œå†ç”¨äºèŠ‚ç‚¹åˆ†ç±»ç­‰ï¼›
- **å°†æ•´ä¸ªå›¾è¡¨ç¤ºæˆä½ç»´ã€å®å€¼ã€ç¨ å¯†çš„å‘é‡å½¢å¼**ï¼Œç”¨æ¥å¯¹æ•´ä¸ªå›¾ç»“æ„è¿›è¡Œåˆ†ç±»ï¼›

å›¾åµŒå…¥çš„æ–¹å¼ä¸»è¦æœ‰ä¸‰ç§ï¼š

- **çŸ©é˜µåˆ†è§£ï¼š**åŸºäºçŸ©é˜µåˆ†è§£çš„æ–¹æ³•æ˜¯å°†èŠ‚ç‚¹é—´çš„å…³ç³»ç”¨çŸ©é˜µçš„å½¢å¼åŠ ä»¥è¡¨è¾¾ï¼Œç„¶ååˆ†è§£è¯¥çŸ©é˜µä»¥å¾—åˆ°åµŒå…¥å‘é‡ã€‚é€šå¸¸ç”¨äºè¡¨ç¤ºèŠ‚ç‚¹å…³ç³»çš„çŸ©é˜µåŒ…æ‹¬é‚»æ¥çŸ©é˜µï¼Œæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ŒèŠ‚ç‚¹è½¬ç§»æ¦‚ç‡çŸ©é˜µï¼ŒèŠ‚ç‚¹å±æ€§çŸ©é˜µç­‰ã€‚æ ¹æ®çŸ©é˜µæ€§è´¨çš„ä¸åŒé€‚ç”¨äºä¸åŒçš„åˆ†è§£ç­–ç•¥ã€‚
- **DeepWalkï¼š**DeepWalk æ˜¯åŸºäº word2vec è¯å‘é‡æå‡ºæ¥çš„ã€‚word2vec åœ¨è®­ç»ƒè¯å‘é‡æ—¶ï¼Œå°†è¯­æ–™ä½œä¸ºè¾“å…¥æ•°æ®ï¼Œè€Œå›¾åµŒå…¥è¾“å…¥çš„æ˜¯æ•´å¼ å›¾ï¼Œä¸¤è€…çœ‹ä¼¼æ²¡æœ‰ä»»ä½•å…³è”ã€‚ä½†æ˜¯ DeepWalk çš„ä½œè€…å‘ç°ï¼Œé¢„æ–™ä¸­è¯è¯­å‡ºç°çš„æ¬¡æ•°ä¸åœ¨å›¾ä¸Šéšæœºæ¸¸èµ°èŠ‚ç‚¹è¢«è®¿é—®åˆ°åº•çš„æ¬¡æ•°éƒ½æœä»å¹‚å¾‹åˆ†å¸ƒã€‚å› æ­¤ DeepWalk æŠŠèŠ‚ç‚¹å½“åšå•è¯ï¼ŒæŠŠéšæœºæ¸¸èµ°å¾—åˆ°çš„èŠ‚ç‚¹åºåˆ—å½“åšå¥å­ï¼Œç„¶åå°†å…¶ç›´æ¥ä½œä¸º word2vec çš„è¾“å…¥å¯ä»¥èŠ‚ç‚¹çš„åµŒå…¥è¡¨ç¤ºï¼ŒåŒæ—¶åˆ©ç”¨èŠ‚ç‚¹çš„åµŒå…¥è¡¨ç¤ºä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„åˆå§‹åŒ–å‚æ•°å¯ä»¥å¾ˆå¥½çš„ä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆæœï¼Œä¹Ÿå‚¬ç”Ÿäº†å¾ˆå¤šç›¸å…³çš„å·¥ä½œï¼›
- **Graph Neural Networkï¼š**å›¾ç»“åˆdeep learningæ–¹æ³•æ­å»ºçš„ç½‘ç»œç»Ÿç§°ä¸ºå›¾ç¥ç»ç½‘ç»œGNNï¼Œä¹Ÿå°±æ˜¯ä¸‹ä¸€å°èŠ‚çš„ä¸»è¦å†…å®¹ï¼Œå› æ­¤å›¾ç¥ç»ç½‘ç»œGNNå¯ä»¥åº”ç”¨äºå›¾åµŒå…¥æ¥å¾—åˆ°å›¾æˆ–å›¾èŠ‚ç‚¹çš„å‘é‡è¡¨ç¤ºï¼›

ä»ä¸€ä¸ªèŠ‚ç‚¹å‡ºå‘, éšæœºè¡Œèµ° 

æœ€ç®€å•çš„æƒ³æ³•æ˜¯å›ºå®šæ­¥é•¿, unbiased random walks starting from each node

ç›®æ ‡:  Embed nodes with similar network neighborhoods close in the feature space.

#### node2vec ç®—æ³•

ï‚¡ 1) Compute random walk probabilities 

ï‚¡ 2) Simulate ğ‘Ÿ random walks of length ğ‘™ starting from each node ğ‘¢ 

ï‚¡ 3) Optimize the node2vec objective ç”¨éšæœºæ¢¯åº¦ä¸‹é™

çº¿æ€§æ—¶é—´å¤æ‚åº¦  ï‚¡ä¸Šé¢ä¸‰æ­¥å¯ä»¥ç‹¬ç«‹å¹¶è¡Œ

PPTé‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–çš„random walk ç®—æ³• 

æ ¸å¿ƒæ€æƒ³: Embed nodes so that distances in embedding space å°±åæ˜ å‡ºèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦.

æ²¡æœ‰ä¸€ç§method å¯ä»¥åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ›´å¥½. å¿…é¡»æ ¹æ®å…·ä½“åº”ç”¨æ¥é€‰æ‹©node ç›¸ä¼¼åº¦çš„ç®—æ³•

#### embedded æ•´ä¸ªå›¾

æ–¹æ³•1  æŠŠæ‰€æœ‰èŠ‚ç‚¹çš„embedding æ±‚å’Œæˆ–è€…æ±‚å¹³å‡,simple but effective

æ–¹æ³•2  å¼•å…¥ä¸€ä¸ªvirtual node

æ–¹æ³•3  åŒ¿åwalk embedding

â–ª Idea 1: Sample the anon. walks and represent the graph as fraction of times each anon walk occurs. â–ª Idea 2: Learn graph embedding together with anonymous walk embeddings.

### lec4 pagerank

In this lecture, we investigate graph analysis and learning from a matrix perspective. 

ï‚¡ Treating a graph as a matrixçš„å¥½å¤„

â–ª ç¡®å®šèŠ‚ç‚¹çš„importance é€šè¿‡ random walk (PageRank) 

â–ª è·å¾—èŠ‚ç‚¹çš„ embeddings via matrix factorizationå› å¼åˆ†è§£ (MF) 

â–ª View other node embeddings (e.g. Node2Vec) as MF 

Random walk, matrix factorization and node embeddings éƒ½æ˜¯ç´§å¯†ç›¸å…³çš„! 

æˆ‘ä»¬è®¨è®ºä¸‹é¢å‡ ä¸ª Link Analysis approaches æ¥è®¡ç®—graphä¸­èŠ‚ç‚¹çš„é‡è¦æ€§: 

1.  PageRank 
2. Personalized PageRank (PPR) 
3. Random Walk with Restarts

#### page rank

åˆ©ç”¨link ç»“æ„æ¥measure èŠ‚ç‚¹çš„é‡è¦æ€§

rank vector r æ˜¯éšæœºé‚»æ¥çŸ©é˜µçš„ä¸€ä¸ªeigenvector: We can now efficiently solve for r! â–ª The method is called Power iteration

å¼€å§‹æ¯ä¸ªé‡è¦æ€§éƒ½æ˜¯1/èŠ‚ç‚¹ä¸ªæ•°,  ç„¶åä¸€æ­¥æ­¥è¿­ä»£, çŸ©é˜µä¹˜æ³•. å¤§çº¦50æ¬¡è¿­ä»£å°±è¶³å¤Ÿ ä¼°è®¡å‡ºæœ‰é™è§£. 

ä¸¤ä¸ªé—®é¢˜ 

1  æœ‰çš„é¡µæ˜¯dead endçš„. è¿™æ˜¯ä¸€ä¸ªé—®é¢˜, matrix ä¸æ˜¯åˆ—éšæœºçš„.

è§£å†³æ–¹æ³•: éšæœºç¬ç§»åˆ°å…¶ä»–æ‰€æœ‰é¡µé¢. å®ç°, å°±æ˜¯è°ƒæ•´é‚»æ¥çŸ©é˜µ.   

2 spider traps , æ‰€æœ‰çš„out linkéƒ½åœ¨ä¸€ä¸ªgroupé‡Œ, æœ€ç»ˆè¿™ä¸ªtrapä¼šå¸æ”¶æ‰€æœ‰çš„importance . æ¯”å¦‚æœ€ç®€å•çš„å°±æ˜¯ä¸€ä¸ªè‡ªç¯èŠ‚ç‚¹.  è¿™å¯¼è‡´scoreä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„. 

è§£å†³æ–¹æ³•: å¼•å…¥æ¦‚ç‡, æ¯ä¸€æ­¥, beta æ¦‚ç‡ éšæœºé€‰ä¸€ä¸ªlinkç„¶åfollow a link , 1-beta æ¦‚ç‡ ç¬ç§»åˆ°éšæœºä¸€é¡µ.  betaé€šå¸¸ 0.8-0.9. 

Personalized PageRank: ç¬ç§»åˆ°ä¸€äº›æŒ‡å®šçš„èŠ‚ç‚¹è€Œä¸æ˜¯æ‰€æœ‰èŠ‚ç‚¹

#### Random Walk with Restarts

ç”¨åœ¨æ¨èç³»ç»Ÿ

èŠ‚ç‚¹çš„proximityæ€ä¹ˆæ¯”è¾ƒ?

Personalized PageRank: â–ª Ranks proximity of nodes to the teleport nodes S

Proximity on graphs: â–ª Q: What is most related item to Ite Q? â–ª Random Walks with Restarts â–ª Teleport back to the starting node: ğ‘º = {Q}

##### éšæœºè¡Œèµ°

Idea  æ¯ä¸ªèŠ‚ç‚¹ä¸€æ ·é‡è¦ â–ª Importance gets evenly split among all edges and pushed to the neighbors: 

ç»™å‡ºæŸ¥è¯¢çš„èŠ‚ç‚¹, å¯ä»¥æ¨¡æ‹Ÿä¸€æ¬¡éšæœºè¡Œèµ°: 

â–ª Make a step éšæœºåˆ°ä¸€ä¸ªé‚»å±…å¹¶ä¸”è®°å½•ä¸‹æ¥(visit count) 

â–ª alphaçš„æ¦‚ç‡, restart the walk at one of the QUERY_NODES 

â–ª æœ‰æœ€å¤šæ¬¡ visit countçš„èŠ‚ç‚¹å°±æœ‰æœ€å¤§çš„proximity to the QUERY_NODES

æ–¹æ³•çš„ä¼˜ç‚¹:  Because the â€œsimilarityâ€ considers: 

â–ª Multiple connections â–ª Multiple paths â–ª Direct and indirect connections â–ª Degree of the node

#### å› å¼åˆ†è§£

DeepWalk and node2vec have a more complex node similarity definition based on random walks 

ï‚¡ DeepWalk is equivalent to matrix factorization of the following complex matrix expression:

é€šè¿‡çŸ©é˜µå› å¼åˆ†è§£å’Œéšæœºè¡Œèµ°ï¼Œ å¯ä»¥æœ‰node embedding  æ¯”å¦‚ deepwalkæˆ– node2vec 

ç¼ºç‚¹ï¼š 

1. ä¸èƒ½è·å¾—ä¸åœ¨è®­ç»ƒé›†ä¸­çš„node embeddingã€‚ éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰node embedding
2. ä¸èƒ½å‘ç°ç»“æ„çš„similarityï¼Œä¼šæœ‰æˆªç„¶ä¸åŒçš„embedding
3. ä¸èƒ½å……åˆ†åˆ©ç”¨node edge graphçš„features

è§£å†³æ–¹æ³•ï¼š  GNNï¼Œ deep representation learning

### lec5 èŠ‚ç‚¹åˆ†ç±»å’Œæ ‡ç­¾ä¼ æ’­

Main question today: ç½‘ç»œæœ‰ä¸€äº›æ ‡ç­¾, æ€ä¹ˆassign æ ‡ç­¾ç»™å…¶ä»–çš„node?

ä¸€ç§æ˜¯lec3æåˆ°çš„node embedding , ä»Šå¤©è®¨è®ºå¦ä¸€ç§æ¡†æ¶: æ¶ˆæ¯ä¼ é€’

å› ä¸ºç½‘ç»œä¸­æœ‰correlation.

æˆ‘ä»¬ä¼šè®¨è®ºä¸‰ä¸ªæŠ€æœ¯   relational åˆ†ç±», iterative åˆ†ç±»å’Œ belief propagation.

homophily: èŠ‚ç‚¹ä¹‹é—´ä¼šç›¸ä¼¼, ç›¸ä¼¼çˆ±å¥½çš„äººä¼šæœ‰æ›´å¤šè”ç³». 

èŠ‚ç‚¹çš„è”ç³», åˆä¼šå½±å“èŠ‚ç‚¹çš„å±æ€§. 

#### åŠç›‘ç£äºŒåˆ†ç±»

 ä¸‰ç§æ–¹æ³•  relationalåˆ†ç±», iterative åˆ†ç±», å’Œ correct and smooth

#####  relationalåˆ†ç±»

 æŠŠé‚»å±…èŠ‚ç‚¹çš„labelåŠ æƒæ±‚å’Œ. 

éš¾ç‚¹:  ä¸æ”¶æ•›, æ¨¡å‹æ²¡æœ‰åˆ©ç”¨èŠ‚ç‚¹çš„featureä¿¡æ¯=> æ‰€ä»¥éœ€è¦iterative åˆ†ç±» 

##### iterative åˆ†ç±» 

è®­ç»ƒä¸¤ä¸ªåˆ†ç±»å™¨, base åˆ†ç±»å™¨æ ¹æ®attributes é¢„æµ‹.  

relational åˆ†ç±»å™¨æ ¹æ®é‚»å±…èŠ‚ç‚¹çš„labelå’Œattributes  é¢„æµ‹label

å¯ä»¥æé«˜ collective åˆ†ç±»

##### correct and smooth

2021å¹´9æœˆSOTAçš„åˆ†ç±»æ–¹æ³•. 

åˆ†ä¸ºä¸‰æ­¥

1. è®­ç»ƒbase é¢„æµ‹å™¨ (å¯ä»¥å¾ˆç®€å•, æ¯”å¦‚å°±MLPå¤šå±‚æ„ŸçŸ¥å™¨)
2. ç”¨baseé¢„æµ‹å™¨æ¥é¢„æµ‹æ‰€æœ‰èŠ‚ç‚¹çš„soft label(æ¯ä¸ªç±» çš„å¯èƒ½æ€§)
3. åˆ©ç”¨å›¾çš„ç»“æ„ ä¿®æ”¹2ä¸­çš„é¢„æµ‹ . åˆ†ä¸ºcorrectå’Œsmooth.  correctçš„åŸç†æ˜¯: ä¸€èˆ¬baseé¢„æµ‹çš„é”™è¯¯å’Œå›¾çš„è¾¹æ˜¯æ­£ç›¸å…³çš„, æ‰€ä»¥ä¸€ä¸ªèŠ‚ç‚¹çš„errorä¼šä¼ æ’­åˆ°å…¶ä»–error. 

### lec6 GNN1 æ¨¡å‹

shallow encoderçš„ç¼ºç‚¹:

1. éœ€è¦ç©ºé—´, æ²¡æœ‰å…±äº«å‚æ•°, æ¯ä¸ªnodeæœ‰è‡ªå·±çš„embedding
2. ä¸èƒ½é¢å¯¹æ–°çš„node, è¾“å…¥çš„æ•°æ®åŒ…å«æµ‹è¯•é›†çš„æ•°æ®,è®­ç»ƒè¿‡ç¨‹èƒ½å¤Ÿçœ‹åˆ°è¿™äº›æ•°æ®,æ‰€ä»¥æ˜¯transductiveç›´æ¨å¼å­¦ä¹  ä¸æ˜¯inductive
3. ä¸èƒ½ç»“åˆnode çš„feature

ç”¨GNN å¤šå±‚, å¯ä»¥è§£å†³åˆ†ç±», é¢„æµ‹link, å…³ç³»æ¢ç´¢, ç½‘ç»œç›¸ä¼¼æ€§ç­‰é—®é¢˜. 

#### æ·±åº¦å­¦ä¹ åŸºç¡€

å¸¸ç”¨çš„åˆ†ç±»loss æ˜¯cross entropy CEäº¤å‰ç†µ 

å­¦ä¹ ç‡ LR, æ˜¯ä¸€ä¸ªè¶…å‚æ•°, æ§åˆ¶gradient stepçš„å¤§å°

å½“ validation setæ²¡æœ‰å˜åŒ–çš„æ—¶å€™,æˆ‘ä»¬å°±åœæ­¢è®­ç»ƒ. 

æ¯ä¸ªç‚¹éƒ½è®¡ç®—å¤ªæ…¢, æ‰€ä»¥ç”¨éšæœºæ¢¯åº¦ä¸‹é™, æ¯ä¸€æ­¥é€‰ä¸åŒçš„minibatch. epoch å°±æ˜¯æ•´ä¸ªdataset å…¨éƒ¨è¿‡äº†ä¸€é. 

SGDæ˜¯full gradient çš„æ— åä¼°è®¡. 

##### å¼•å…¥éçº¿æ€§

ReLU,  rectified linear unit, å°±æ˜¯ max(x,0)

sigmoid , å°±æ˜¯ 1/(1+e^-x)  å€¼åŸŸåœ¨ 0åˆ°1 

### ç¨€ç–çŸ©é˜µ

 https://en.wikipedia.org/wiki/Sparse_matrix

COO ,COO stores a list of (row, column, value) tuples. Ideally, the entries are sorted first by row index and then by column index, to improve random access times. This is another format that is good for incremental matrix construction

Formats can be divided into two groups:

- Those that support efficient modification, such as DOK (Dictionary of keys), LIL (List of lists), or COO (Coordinate list). These are typically used to construct the matrices.
- Those that support efficient access and matrix operations, such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).

#### å›¾æ·±åº¦å­¦ä¹ 

##### naive approach

ä¸´æ¥çŸ©é˜µç›´æ¥é™„åŠ ä¸ŠfeatureçŸ©é˜µ.  feed into DNN, é—®é¢˜æ˜¯

1. å‚æ•°å¤š,  å‚æ•°æ¯”training example æ›´å¤š, è¿™æ ·è®­ç»ƒä¸ç¨³å®š,  å®¹æ˜“è¿‡æ‹Ÿåˆ
2.   graphå¤§å°ä¸åŒ, æ¯”å¦‚7èŠ‚ç‚¹, è®­ç»ƒçš„äº”ä¸ªèŠ‚ç‚¹çš„æ¨¡å‹å°±ä¸é€‚ç”¨.
3. å›¾æ˜¯æ²¡æœ‰å›ºå®šé¡ºåºçš„, èŠ‚ç‚¹çš„é¡ºåºå˜äº†, å°±ä¸ä¸€æ ·äº†. 

åˆ©ç”¨CNN ? å›¾æ˜¯éå¸¸å¤šå˜çš„, æ²¡æœ‰å›ºå®šçš„æ»‘åŠ¨çª—å£æ¥å·ç§¯. 

æ‰€ä»¥æå‡ºäº†æ–°çš„ç½‘ç»œ: 

#### å›¾å·ç§¯ç½‘ç»œGCN

æ¯ä¸ªèŠ‚ç‚¹æ ¹æ®é‚»å±…, å®šä¹‰äº†ä¸€ä¸ªè®¡ç®—å›¾ .

 è¦åŒæ—¶è®­ç»ƒæ¯ä¸ªèŠ‚ç‚¹çš„è®¡ç®—å›¾.  

æ€ä¹ˆå®ç°èŠ‚ç‚¹é¡ºåºå˜äº†è¾“å‡ºä¸å˜çš„?  æ“ä½œçš„ç½®æ¢ä¸å˜æ€§

##### matrix formulation

è®­ç»ƒçš„å‚æ•°æ˜¯ä»€ä¹ˆ? 

è®¸å¤š aggregations å¯ä»¥é€šè¿‡ç¨€ç–çŸ©é˜µçš„è¿ç®—åŠ é€Ÿ.

æœ‰è®²åˆ°degreeä¸€æ ·çš„sparse matrix multiplicationå—? å¥½åƒæ²¡æœ‰

ä¸æ˜¯æ‰€æœ‰GNNéƒ½å¯ä»¥è¡¨ç¤ºæˆçŸ©é˜µå½¢å¼, æ¯”å¦‚aggregation å‡½æ•°ç‰¹åˆ«å¤æ‚çš„æ—¶å€™. 

The same aggregation parameters å¯ä»¥è¢«æ‰€æœ‰èŠ‚ç‚¹åˆ†äº«. è€Œä¸”æ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°æ²¡è§è¿‡çš„èŠ‚ç‚¹. 

**å¹³å‡æ³•**

æ¯ä¸ªèŠ‚ç‚¹, æ”¶é›†æ¥è‡ªé‚»å±…ä¼ é€’çš„ä¿¡æ¯, ç„¶åæ±‡æ€»åæ›´æ–°è‡ªå·±.

æ¯ä¸ªèŠ‚ç‚¹å’Œå®ƒé‚»å±…éƒ½æ˜¯ç›¸ä¼¼çš„, é‚£ä¹ˆæ¯ä¸ªèŠ‚ç‚¹å°±ç­‰äºé‚»å±…èŠ‚ç‚¹çš„å¹³å‡å€¼.

**åŠ æƒå¹³å‡æ³•**

æ¯ä¸ªèŠ‚ç‚¹å’Œé‚»å±…çš„å…³ç³»å¼ºåº¦æ˜¯ä¸åŒçš„, è€ƒè™‘åˆ°è¾¹çš„æƒé‡å…³ç³», åªéœ€è¦å°†é‚»æ¥çŸ©é˜µ A å˜ä¸ºæœ‰æƒå›¾

**æ·»åŠ è‡ªå›ç¯**

å‰é¢æåˆ°çš„å¹³å‡æ³•, åŠ æƒå¹³å‡æ³•éƒ½å¿½ç•¥äº†è‡ªèº«èŠ‚ç‚¹çš„ç‰¹å¾, æ•…åœ¨æ›´æ–°è‡ªèº«èŠ‚ç‚¹æ—¶, ä¸€èˆ¬ä¼šæ·»åŠ ä¸ªè‡ªç¯,æŠŠè‡ªèº«ç‰¹å¾å’Œé‚»å±…ç‰¹å¾ç»“åˆæ¥æ›´æ–°èŠ‚ç‚¹

**å½’ä¸€åŒ–**

ä¸åŒèŠ‚ç‚¹, å…¶è¾¹çš„æ•°é‡å’Œæƒé‡å¹…å€¼éƒ½ä¸ä¸€æ ·, æ¯”å¦‚æœ‰çš„èŠ‚ç‚¹ç‰¹åˆ«å¤šè¾¹, è¿™å¯¼è‡´å¤šè¾¹(æˆ–è¾¹æƒé‡å¾ˆå¤§)çš„èŠ‚ç‚¹åœ¨èšåˆåçš„ç‰¹å¾å€¼è¿œè¿œå¤§äºå°‘è¾¹(è¾¹æƒé‡å°)çš„èŠ‚ç‚¹. æ‰€ä»¥éœ€è¦åœ¨èŠ‚ç‚¹åœ¨æ›´æ–°è‡ªèº«å‰, å¯¹é‚»å±…ä¼ æ¥çš„ä¿¡æ¯(åŒ…æ‹¬è‡ªç¯ä¿¡æ¯)è¿›è¡Œå½’ä¸€åŒ–æ¥æ¶ˆé™¤è¿™é—®é¢˜

#### GNN CNNå’Œtransformer

CNNå¯ä»¥çœ‹ä½œä¸€ç§ç‰¹æ®Šçš„GNN, é‚»å±…çš„å¤§å°æ˜¯å›ºå®šçš„, ordering ä¹Ÿæ˜¯å›ºå®šçš„. 

CNNä¸æ˜¯ç­‰ä»·äº¤æ¢permutatinoçš„, æ”¹å˜åƒç´ çš„é¡ºåºä¼šæœ‰ä¸åŒçš„è¾“å‡º. 

##### transformer

NLPå¾ˆæœ‰ç”¨, åºåˆ—å¤„ç†çš„é—®é¢˜ä¸Šæ˜¯æœ€å—æ¬¢è¿çš„ä¸€ç§æ¨¡å‹. 

transformer ä¹Ÿå¯ä»¥çœ‹ä½œä¸€ç§ç‰¹æ®Šçš„GNN, æ˜¯åœ¨ä¸€ä¸ªå…¨è¿æ¥çš„wordå›¾ä¸Š. 

### lec7 GNN2 Design Space 

ç”¨pyG è§£å†³ä¸€ä¸ªå®é™…é—®é¢˜. å‘å¸ƒä¸€ä¸ªblog post. 

##### ä»€ä¹ˆæ˜¯å¥½çš„åšå®¢?

ä¸€æ­¥æ­¥è§£é‡Š å›¾å­¦ä¹ æŠ€æœ¯, å‡è®¾ä½ çš„è¯»è€…ç†Ÿæ‚‰ML, ä½†æ˜¯ä¸ç†Ÿæ‚‰ PyG

å¯è§†åŒ–, gif > image > Text , å¯è§†åŒ–è¶Šå¤šè¶Šå¥½

code snippetsè¦æœ‰,   é“¾æ¥åˆ°colab æ¥å¤ç°ç»“æœ. 

åº”ç”¨åœ¨å“ªé‡Œ?

æ¨èç³»ç»Ÿ, åˆ†å­åˆ†ç±», è®ºæ–‡åˆ†ç±», çŸ¥è¯†å›¾è°±, äº§å“è´­ä¹°åˆ†ç±», è›‹ç™½è´¨ç»“æ„

åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°æ¨¡å‹? OGB leaderboard å’Œtop ML ä¼šè®®:  KDD, ICLR, ICML , neurlPS WWW, WSDM 

deep graph encoder , å°±æ˜¯GCN , ç„¶åactivation function, ç„¶å æ­£åˆ™åŒ–, æ¯”å¦‚dropout,  ç„¶åGCN, 

every node base on neighborç¡®å®šäº†ä¸€ä¸ªè®¡ç®—å›¾, å¯ä»¥ä¼ æ’­ä¿¡æ¯æ¥è®¡ç®—node feature .  aggregate ä¿¡æ¯. 

GNN Layer = Message + Aggregation   ä¸€å±‚GNN layer , å°±æ˜¯æŠŠä¸€äº›vectors å‹ç¼©æˆä¸€ä¸ªvector . 

message å¯ä»¥ç”¨ä¸€ä¸ªçº¿æ€§å±‚, aggregation å¯ä»¥ç”¨max, meanæˆ–è€…sum 

æ€ä¹ˆç”¨ä»£ç å†™å‡ºè¿™äº›å‘¢? colab3 çš„éš¾ç‚¹å°±åœ¨è¿™é‡Œ. 

##### message è®¡ç®—

å¯¹äºæ‰€æœ‰é‚»å±… node è®¡ç®—å‡ºmessage. 

ä¾‹å¦‚, ä¸€ä¸ªlinear layer, æŠŠnode feature [num node, node feature ]çŸ©é˜µä¹˜ä¸Šä¸€ä¸ªæƒé‡çŸ©é˜µ. 

##### aggregation

ä»é‚»å±…é‚£é‡Œèšåˆmessage. `sum( [ message for message in neighbor ] )`

ä¾‹å¦‚,  sum,  max, min å°±æ˜¯aggregation. 

##### è€ƒè™‘è‡ªå·±

é™¤äº†é‚»å±…è¿˜è¦è€ƒè™‘è‡ªå·±çš„ä¿¡æ¯, è¿™æ—¶message ç”¨å¦ä¸€ç§å‡½æ•°è®¡ç®—, aggregation, ç”¨concatenationä¸²è” æˆ–è€…summation

##### éçº¿æ€§

 activation , å¯ä»¥ add expressiveness, ä¸€èˆ¬ç”¨ReLUæˆ–  Sigmoid .  åŠ åˆ°messageæˆ–aggregationéƒ½å¯ä»¥. 

#### ç»å…¸çš„GNN å±‚

##### GCN

message:  ä¹˜ä¸Šä¸€ä¸ªæƒé‡çŸ©é˜µç„¶åæ ¹æ®å…¥åº¦æ­£åˆ™åŒ–

aggregation:  ç”¨sum, ç„¶åactivation

##### GraphSAGE

æœ‰ä¸¤ä¸ªè´¡çŒ®ï¼Œä¸€æ˜¯æå‡ºäº†æ›´å¤šçš„èšåˆæ–¹æ³•ï¼ˆmean/lstm/poolingï¼‰ï¼ŒäºŒæ˜¯å¯¹é‚»å±…ä¿¡æ¯è¿›è¡Œå¤šè·³æŠ½æ ·

message ç”¨AGGè®¡ç®—, aggregationåˆ†ä¸¤ä¸ªstage ,å…ˆagg é‚»å±…,å†aggè‡ªå·±. 

`æƒé‡çŸ©é˜µ ä¸²è”( è‡ªå·±,(èšåˆé‚»å±…))`

L2æ­£åˆ™åŒ–, å¯ä»¥åœ¨æ¯ä¸€æ¬¡apply, ä½œç”¨æ˜¯æœ‰ä¸€è‡´çš„scales, æœ‰æ—¶å€™å¯ä»¥æé«˜performance. 

##### Graph Attention Networks

GCN å’Œ GraphSAGEä¸­, æ¯ä¸ªé‚»å±…æ˜¯ä¸€æ ·é‡è¦çš„, æƒé‡ç³»æ•°å’Œå›¾çš„ç»“æ„å±æ€§æ­£ç›¸å…³(å…·ä½“çš„æ¥è¯´, å’ŒèŠ‚ç‚¹çš„åº¦æ­£ç›¸å…³)

GATä¸­,  attentionå…³æ³¨é‡è¦çš„ä¿¡æ¯, è®¡ç®—ä¸€ä¸ªattention coefficient,  normalize åå¾—åˆ°æœ€ç»ˆçš„æƒé‡. åŠ æƒæ±‚å’Œ.

multi-headæ˜¯åœ¨channelä¸Šè¿›è¡Œåˆ‡åˆ†åˆ†åˆ«è®¡ç®—æœ€åæ‹¼æ¥/å¹³å‡

ä¼˜ç‚¹: è®¡ç®—é«˜æ•ˆ, è®¡ç®— attention coefficientå¯ä»¥å¹¶è¡Œ, å­˜å‚¨æ›´é«˜æ•ˆ, å‚æ•°å›ºå®šä¸å†å’Œå›¾å¤§å°å¢é•¿, 

æ¯”å¦‚ batch normalizationæ¥ç¨³å®šè®­ç»ƒ, dropoutæ¥é˜²æ­¢è¿‡æ‹Ÿåˆ, attention/gateingæ¥æ§åˆ¶ä¸€ä¸ªæ¶ˆæ¯çš„é‡è¦æ€§

##### batch normalization

dropout éšæœºæŠŠä¸€äº›ç¥ç»å…ƒç½®ä¸º0 

GNNä¸­ , linear layer  message function åº”ç”¨dropout 

#### å±‚æ¬¡ç›¸è¿é—®é¢˜

##### å¤ªå¤šGNNå±‚é—®é¢˜

 over smoothing, æ‰€æœ‰embedding æ”¶æ•›åˆ°åŒä¸€ä¸ªå€¼.  ä¸ºä»€ä¹ˆä¼šè¿™æ ·?

å †å å¤ªå¤šå±‚- >  receptive åŸŸé«˜åº¦é‡å , ä¹Ÿå°±æ˜¯å¤§å®¶hopè¿‡æ¥éƒ½æ˜¯è¿™äº›èŠ‚ç‚¹ -> node embedding é«˜åº¦ç›¸ä¼¼ 

##### è§£å†³æ–¹æ³•

1.ä¸è¦åŠ å¤ªå¤šå±‚, åˆ†æå¿…è¦çš„receptive field. 

é‚£æ€ä¹ˆå…·æœ‰è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›å‘¢?  åœ¨æ¯å±‚å†…ä¿®æ”¹. 

2. å¢åŠ ä¸ä¼ é€’message çš„layer 

ä¸€ä¸ªGNN ä¸ä¸€å®šå…¨æ˜¯GNN layer  , å¯ä»¥åœ¨å‰é¢åé¢åŠ ä¸€äº›MLP layer .  å®é™…ä¸­å¾ˆæœ‰ç”¨. 

å½“encodingå¾ˆé‡è¦çš„æ—¶å€™,  æ¯”å¦‚nodeè¡¨ç¤ºimages/texts æ—¶, å°±åŠ pre processing layers. 

reasoning/transformation é‡è¦çš„æ—¶å€™,  æ¯”å¦‚graph classification, knowledge graphs,  åŠ Post-processing layers

å¦‚æœå¿…é¡»éœ€è¦å¾ˆå¤šå±‚å‘¢?

é‚£å°±add skip connection: increase the impact of earlier layers on the final node embeddings, åœ¨GNNä¸­å¢åŠ shortcut

å¯ä»¥æœ‰æ›´å¤šå¯èƒ½æ€§, å°±æœ‰æ··åˆ æµ…GNN å’Œæ·±GNN . 

###  lec8 augmentation 

ä¸ºä»€ä¹ˆè¦å¢å¼ºå›¾?

1. features , ç¼ºä¹features 
2. å›¾ç»“æ„, å¯èƒ½å¤ªsparse å¯¼è‡´æ¶ˆæ¯ä¼ é€’å¤ªæ²¡æ•ˆç‡æˆ–è€…å¤ªdenseå¯¼è‡´æ¶ˆæ¯ä¼ é€’å¼€é”€å¤ªå¤§, å¤ªlarge å¯¼è‡´æ˜¾å­˜ä¸å¤Ÿ.  

æ‰€ä»¥, è¾“å…¥çš„å›¾å¾€å¾€ä¸æ˜¯æœ€é€‚åˆçš„è®¡ç®—å›¾

#### å¢å¼ºçš„æ–¹æ³•

##### ç‰¹å¾å¢å¼º

å¯èƒ½æ²¡æœ‰node feature

æ ‡å‡†çš„æ–¹æ³•æ˜¯

1. ç»™nodesåˆ†é…å¸¸é‡å€¼. 
2. ç»™nodeåˆ†é…unique ID, one hot vectors, è¿™ä¸ªæ–¹æ³•æ›´expressive, ä½†æ˜¯ä¸èƒ½æ³›åŒ–åˆ°unseen node, è®¡ç®—çš„å¼€é”€ä¹Ÿæ›´å¤§, å¯ä»¥ç”¨åœ¨å°å›¾å’Œtransductiveå›¾(æ²¡æœ‰æ–°çš„node).  

å¯èƒ½GNN å­¦ä¸åˆ°ä¸€äº›ç‰¹å®šçš„ç»“æ„, æ¯”å¦‚ç¯çš„é•¿åº¦ 

å­¦ä¸åˆ°å°±ç›´æ¥åŠ è¿›å», æ¯”å¦‚ Node degree Â§ Clustering coefficient Â§ PageRank Â§ Centrality

##### ç»“æ„å¢å¼º

**å¤ªsparse  ->  Add virtual nodes / edges**

æ€ä¹ˆåŠ è¾¹?  Connect 2-hop neighbors via virtual edges  ä¾‹å­æ˜¯Bipartite graphs

åŠ ä¸€ä¸ªè™šæ‹Ÿnode, è¶…çº§æºç‚¹. è¿™æ ·æ‰€æœ‰èŠ‚ç‚¹éƒ½è·ç¦»ä¸º2 è¿™æ ·å¯ä»¥å¤§å¤§åŠ å¿«ç¨€ç–çŸ©é˜µä¸­çš„æ¶ˆæ¯ä¼ é€’. 

**å¤ªdense  -> Sample neighbors when doing message passing**

éšæœºé‡‡æ ·ä¸€äº›èŠ‚ç‚¹çš„é‚»å±…, æœ‰ä¸€äº›é‚»å±…ä¸è®¡ç®—äº†, æ¥å‡å°‘è®¡ç®—é‡. åœ¨å®è·µä¸­ä¹Ÿworkçš„å¾ˆå¥½

**å¤ªlarge  -> é‡‡æ ·å­å›¾** 

å¯ä»¥æœ‰minibatch,  (é‡‡æ ·æ—¶é—´, cpuç§»åŠ¨åˆ°gpuæ—¶é—´)å¾ˆä¹…. ç”šè‡³å¯ä»¥åˆ°85%.  ä¸åŒçš„é‡‡æ ·æ–¹æ³•é€Ÿåº¦ä¹Ÿä¸ä¸€æ ·. 

æ¯”å¦‚æŠŠä¸€å¼ å¤§å›¾, åˆ†æˆ15000ä¸ªcluster, ä¸€ä¸ªmini batch è®­ç»ƒ32ä¸ªå­å›¾, 

å¯è§æ€§, 

#### ç”¨GNNé¢„æµ‹

3ä¸ªlevel , èŠ‚ç‚¹, edge, graph. 

global pooling ä¼šä¸¢å¤±ç»“æ„ä¿¡æ¯.  -> è§£å†³æ–¹æ³•, åˆ†å±‚aggregate æ‰€æœ‰çš„node embedding 

å®è·µä¸­å¯ä»¥ç”¨ diffpool :  åˆ©ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„GNN, åˆ†å±‚è®¡ç®—.  è”åˆè®­ç»ƒ GNN1å’ŒGNN2

regressionå°±æ˜¯labelæœ‰è¿ç»­çš„å€¼, åˆ†ç±»æ˜¯labelç¦»æ•£çš„. 

åˆ†ç±»å¯ä»¥ç”¨CE äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°, 

regressionå°±ç”¨ mean squared error MSE  , ä¹Ÿå«L2 loss 

##### æ•°æ®é›†åˆ†å‰²

Fixed split: åªåˆ‡åˆ†ä¸€æ¬¡

å›¾çš„æ•°æ®é›†åˆ‡åˆ†å¾ˆç‰¹æ®Š, ä¼šå½±å“message passing- > å½±å“èŠ‚ç‚¹çš„embedding

è§£å†³æ–¹æ³•:

1.  transductive setting, è¾“å…¥å›¾å¯ä»¥è¢« æ‰€æœ‰splits çœ‹åˆ°, åªåˆ†å‰²èŠ‚ç‚¹çš„label. åœ¨è®­ç»ƒæ—¶ç”¨æ•´ä¸ªå›¾è®¡ç®—embedding,ç„¶åç”¨éƒ¨åˆ†labelè®­ç»ƒ.  è¯„ä¼°æ—¶ä¹Ÿæ˜¯, æ•´ä¸ªå›¾è®¡ç®—embedding, ç„¶å ç”¨éƒ¨åˆ†label evaluate.
2.  Inductive setting, ä¸åŒsplitä¹‹é—´çš„è¾¹åˆ‡æ‰, ä¸€ä¸ªsplitå°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å›¾, åœ¨è®­ç»ƒæ—¶ç”¨å•ä¸ªsplitçš„å›¾è®¡ç®—embedding,ç„¶åç”¨è¿™ä¸ªsplitçš„labelè®­ç»ƒ. validationä¹Ÿæ˜¯. 

åœ¨è®­ç»ƒ, éªŒè¯å’Œæµ‹è¯•setä¸­éƒ½èƒ½çœ‹åˆ°æ•´ä¸ªå›¾. å«åštransductive setting.  æˆ‘ä»¬åªåˆ†å‰²æ ‡ç­¾. transductive setting åªèƒ½ç”¨äº node/edge é¢„æµ‹ä»»åŠ¡

æˆ–è€… inductive setting, æŠŠè¾¹ä¹Ÿæ‹†å¼€. è·å¾—å¤šä¸ªç‹¬ç«‹çš„å›¾. ç”¨ç‹¬ç«‹çš„å›¾å„è‡ªè®¡ç®—åµŒå…¥.  inductiveå¯ä»¥ç”¨äºèŠ‚ç‚¹/è¾¹/ å›¾ä»»åŠ¡. æ¯”å¦‚ å›¾åˆ†ç±», å› ä¸ºå¿…é¡»testæ²¡è§è¿‡çš„å›¾.  ç”¨æ•´ä¸ªå›¾è®¡ç®—åµŒå…¥, ç”¨å‡ ä¸ªèŠ‚ç‚¹ train, ç„¶å å¦å¤–å‡ ä¸ªèŠ‚ç‚¹evaluate.  è¿™ç§æ˜¯ä¸èƒ½å¤„ç†å›¾é¢„æµ‹ä»»åŠ¡çš„. 

##### é¢„æµ‹edge

Link Predict

è¿™æ˜¯æ— ç›‘ç£/è‡ªç›‘ç£ä»»åŠ¡, è¦è‡ªå·±åˆ›å»ºlabel.  åˆ†å‰²è¾¹ä¸¤æ¬¡

step1 : è¾¹åˆ†ä¸ºä¸¤ç§

inductive : æŠŠè¾¹åˆ†æˆä¸¤ç§ç±»å‹, message edge å’Œsupervision edges . supervision edgeä¸è¾“å…¥GNN . è¿™ä¸¤ä¸ªæ˜¯æ€ä¹ˆåˆ†çš„? 

 message edges(ç”¨æ¥æ¶ˆæ¯ä¼ é€’)å’Œsupervision edges(ç”¨æ¥è®¡ç®—) supervision edges ä½œä¸ºmodel åšå‡ºé¢„æµ‹çš„label, ä¸ä¼šè¢«fed into GNN.  

step2: edge split

transductive : æ˜¯é»˜è®¤çš„è®¾ç½®, æŠŠè¾¹åˆ†æˆå››ç§, Training message edges ,Training supervision edges, Validation edges ,Test edges.  è¿™æ˜¯éå¸¸trickå’Œå¤æ‚çš„, ä¸è¿‡PyG, DeepSNAPå’ŒGraphGym æœ‰å¾ˆå®Œå–„çš„åº“

DeepSNAP provides core modules for é¢„æµ‹ pipeline 

GraphGym è¿›ä¸€æ­¥å®ç°äº†æ•´ä¸ª pipeline to facilitate GNN design

#### å‡ºç°é—®é¢˜äº†æ€ä¹ˆè§£å†³

general tips:

1. æ•°æ®å¤„ç†å¾ˆé‡è¦, å› ä¸ºnode attributes æœ‰äº›å¾ˆå¤§å¯èƒ½å‡ åƒå‡ ä¸‡, éœ€è¦ç”¨æ­£åˆ™åŒ– .attributeæ˜¯å±æ€§å§ï¼Œæ¯”å¦‚degreeï¼Œä¸­å¿ƒæ€§è¿™äº›.
2. ä¼˜åŒ–å™¨ ,ADAM æ˜¯ç›¸å¯¹æ¯”è¾ƒrobustçš„
3. æ¿€æ´»å‡½æ•°, ReLU æ¯”è¾ƒå¥½,Â§ Other alternatives: LeakyReLU, SWISH, rational activation,  è¾“å‡ºå±‚ä¸è¦ç”¨æ¿€æ´»å‡½æ•°
4. æ¯ä¸€å±‚åŒ…å«bias term
5. embedding ç»´åº¦, 32 , 64 , 128 æ¯”è¾ƒå¥½

##### æ€ä¹ˆdebug

loss/accuracy ä¸æ”¶æ•›: 

1. æ£€æŸ¥pipeline, æ¯”å¦‚æœ‰æ²¡æœ‰zero grad 
2. è°ƒæ•´è¶…å‚æ•°
3. æ£€æŸ¥æƒé‡å‚æ•°åˆå§‹åŒ–

è¿‡æ‹Ÿåˆ: è°ƒè¯•loss func, å°è¯•å¯è§†åŒ–

### lec9 GNNç†è®º

è¿™äº›GNN æ¨¡å‹,  GCN, GAT, GraphSAGE, design space.  

expressive power (ability to distinguish different graph structures) æœ‰ä»€ä¹ˆåŒºåˆ«?

æ€ä¹ˆè®¾è®¡ä¸€ä¸ªæœ€expressive çš„GNN model?

GCN (mean-pool) [Kipf and Welling ICLR 2017]  Element-wise mean pooling + Linear + ReLU non-linearity

GraphSAGE (max-pool) [Hamilton et al. NeurIPS 2017]MLP + element-wise max-pooling

GNNæ˜¯æ€ä¹ˆæ•æ‰ local neighborhood structures çš„?

å…³é”®æ˜¯è®¡ç®—å›¾

ä¸åŒçš„local neighborhoods  å†³å®šäº†ä¸åŒçš„è®¡ç®—å›¾

å•å°„injective .  å½“*a* â‰  *b*æ—¶ï¼Œ*f*(*a*) ä¸€å®šâ‰  *f*(*b*)

æœ€æœ‰expressive çš„GNN åº”è¯¥map subtrees to the node embeddings injectively. 

å¦‚æœæ¯ä¸€æ­¥neighbor aggregationéƒ½æ˜¯å•å°„çš„, é‚£ä¹ˆGNN å°±å¯ä»¥å®Œå…¨åˆ†è¾¨ä¸åŒçš„å­æ ‘ç»“æ„. 

#### è®¾è®¡æœ€å¼ºå¤§çš„GNN

å¦‚ä¸Šæ‰€è¿°, æœ€é‡è¦çš„å°±æ˜¯injective aggregation function. 

GCN mean , GraphSAGE max éƒ½ä¸æ˜¯å•å°„çš„.  æ‰€ä»¥ä¸å¤Ÿpowerful

Graph Isomorphism Network (GIN) [Xu et al. ICLR 2019]   

Apply an MLP, element-wise sum, followed by another MLP. æ˜¯å•å°„çš„, is THE most expressive GNN in the class of message-passing GNNs!  

GIN ç”¨ NN to model å•å°„å“ˆå¸Œå‡½æ•°

GIN å’Œ WL graph kernel çš„expressive å·®ä¸å¤š, 

### lec10 Knowledge Graph Embeddings

#### å¼‚è´¨å›¾

æ¯ä¸ªè¾¹çš„ç±»å‹å¯èƒ½ä¸åŒ

##### Relational GCNs 

ä½¿ç”¨ä¸åŒçš„NN æƒé‡æ¥è¡¨ç¤ºä¸åŒçš„relation ç±»å‹. 

##### Knowledge Graphs 

##### Embeddings for KG Completion



### lec16 advanced topic



Recent blog on GNN: [https://blogs.nvidia.com/blog/2022/10/24/what-are-graph-neural-networks/](https://urldefense.com/v3/__https://blogs.nvidia.com/blog/2022/10/24/what-are-graph-neural-networks/__;!!Nmw4Hv0!xeTdkIw9Cr8YgoJy02Y0PdTOsZs-uFQmMWrq3PcwV77vl-bKi_hwHwMEKjkcl3Ug9J4i9uaZk_xH0pCZ6CON1A$)
NVIDIA GNN accelerated software: [https://developer.nvidia.com/gnn-frameworks](https://urldefense.com/v3/__https://developer.nvidia.com/gnn-frameworks__;!!Nmw4Hv0!xeTdkIw9Cr8YgoJy02Y0PdTOsZs-uFQmMWrq3PcwV77vl-bKi_hwHwMEKjkcl3Ug9J4i9uaZk_xH0pAS1slS5A$)
